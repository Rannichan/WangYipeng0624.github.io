<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="NLP,Attention,Transformer," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="Today, I am going to introduce a very interesting paper from Google. You can find the original paper “Attention is All You Need” on the website. In this paper, genius aithors propose new sequence tran">
<meta name="keywords" content="NLP,Attention,Transformer">
<meta property="og:type" content="article">
<meta property="og:title" content="Attention and Transformer">
<meta property="og:url" content="http://wangyp.tech/2019/01/05/Transformer/index.html">
<meta property="og:site_name" content="Yipeng&#39;s Blog">
<meta property="og:description" content="Today, I am going to introduce a very interesting paper from Google. You can find the original paper “Attention is All You Need” on the website. In this paper, genius aithors propose new sequence tran">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://wangyp.tech/2019/01/05/Transformer/1.jpg">
<meta property="og:image" content="http://wangyp.tech/2019/01/05/Transformer/2.jpg">
<meta property="og:image" content="http://wangyp.tech/2019/01/05/Transformer/3.jpg">
<meta property="og:image" content="http://wangyp.tech/2019/01/05/Transformer/4.jpg">
<meta property="og:updated_time" content="2019-01-14T04:31:50.691Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Attention and Transformer">
<meta name="twitter:description" content="Today, I am going to introduce a very interesting paper from Google. You can find the original paper “Attention is All You Need” on the website. In this paper, genius aithors propose new sequence tran">
<meta name="twitter:image" content="http://wangyp.tech/2019/01/05/Transformer/1.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.2',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://wangyp.tech/2019/01/05/Transformer/"/>





  <title>Attention and Transformer | Yipeng's Blog</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Yipeng's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Set yourself on fire</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-tags" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://wangyp.tech/2019/01/05/Transformer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Wang Yipeng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/myavatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yipeng's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Attention and Transformer</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-01-05T21:16:24-08:00">
                2019-01-05
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>Today, I am going to introduce a very interesting paper from Google. You can find the original paper “Attention is All You Need” on the <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">website</a>. In this paper, genius aithors propose new sequence transduction model totally based on attention, called <strong>Transformer</strong>. This model outperforms many previous model based on RNNs, CNNs and shows <strong>attention machenism</strong>‘s importance in modeling sequence.  </p>
<h1 id="Review"><a href="#Review" class="headerlink" title="Review"></a>Review</h1><p><em>Keyword : RNN, LSTM, GRU, Long-term dependency, Parallel Computing.</em></p>
<p>Before talking about attention and transformer, I would first have look of RNNs, LSTMs, GRUs and what they are good at and what not. RNN (recurrent neural network) was proposed to model sequence input. It can capture the relation between current input and the previous input. It is a kind of imitation of the nature that human form a sentence from left to right (jsut an assumption, how human generate a sentence must be more complicated). However, RNN still has drawbacks. Because of the recurrent structure of RNN, the gradient (when we using gradient descent to train an RNN) is easy to vanish or explode after several time steps (recurrences). This is what LSTMs and GRUs want to solve. Upon the original version of RNN, They introduce a structure called <strong>“GATE”</strong> to control the “forget” (of previous input) and “remember” (of current input). Gradient is not that easy to vanish or explode when traning LSTMs and GRUs. Therefore, LSTMs and GRUs can capture long-term dependency and work better than orginal RNN when modeling long sequences. But LSTMs and GRUs are still not perfect. In classic encoder-decoder models, people like to use LSTMs and GRUs to encode input sequence into a fixed-length vector. However, It is very difficult for LSTMs or GRUs to capture enough information of a sequence. </p>
<blockquote>
<p>A potential issue with this encoder–decoder approach is that a neural network needs to be able to<br>compress all the necessary information of a source sentence into a fixed-length vector. This may<br>make it difficult for the neural network to cope with long sentences, especially those that are longer<br>than the sentences in the training corpus.    </p>
<p><a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">Dzmitry Bahdanau, et al. Neural machine translation by jointly learning to align and translate, 2015</a> </p>
</blockquote>
<p><strong>Attention machenism allow decoder to directly look into source sequence, thereby solving the problem of no enough information.</strong></p>
<p>Another disadvantage of RNN (including LSTM, GRU) is its sequential nature preclude parallelization within training. Same case of encoder-decoder model, if we want to make the training parallelizable but don’t build encoder and decoder using RNNs, what we could use? A powerful weapon is Self-Attention. <strong>Self-Attention can capture inter-denpendncy within a sequence, meanwhile, most computing during training is parallelizable.</strong> </p>
<h1 id="Attention-Machenism"><a href="#Attention-Machenism" class="headerlink" title="Attention Machenism"></a>Attention Machenism</h1><p>In general, </p>
<blockquote>
<p>An attention function can be described as mapping a <strong>query</strong> and a set of <strong>key-value pairs</strong> to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a <strong>compatibility function</strong> of the<br>query with the corresponding key.</p>
<p><a href="http://papers.nips.cc/paper/7181-attention-is-all-you-need" target="_blank" rel="noopener">Vaswani, Ashish, et al. “Attention is all you need.” <em>Advances in Neural Information Processing Systems</em>. 2017.</a></p>
</blockquote>
<p>In the case of encoder-decoder model, output of each decoder unit is a query, and output of each encoder unit is a key and corresponding value (here key and corresponding are exactly the same thing). The attention distribution over the encoder outputs describes how an output of decoder unit compatible with each part of encoder’s outputs. Then decoder could directly gather information from encoder’s outputs by computing weighted sum of them, where the weight comes from the attention distribution.</p>
<img src="/2019/01/05/Transformer/1.jpg">
<p>If we want to apply attention machenism in our models, we need to determine <strong>WHERE</strong> and <strong>HOW</strong>. WHERE means ‘from what queries to what keys’ and we are going to talk about it in the next section. HOW means ‘how to compute compatibility between queries and keys’. And this paper mentioned 4 ways:</p>
<ul>
<li><p><strong>Dot-product attention</strong>: Simply compute dot products of a query with all keys, and apply a softmax function to obtain weights on the values.</p>
</li>
<li><p><strong>Scaled dot-product attention</strong>: Queries and keys are of dimension $d_k$. First compute dot products of a query with all keys, and devide each by $\sqrt{d_k}$. Then apply a softmax function to obtain weights on the values.</p>
<script type="math/tex; mode=display">
Attention(Q,K,V)=softmax(QK^T/\sqrt{d_k})V</script></li>
<li><p><strong>Multi-head attention</strong>: Use several learned projection layer to project a set of queries, keys, values into different subspaces. Then apply Scaled dot-product attention on the set of queries, keys, values in each different subspace. Concatenate all attention outputs in different subspaces. At last  use a projection layer to project the concatenation result into the dimension we want.</p>
<script type="math/tex; mode=display">
MultiHead(Q,K,V) = Concat(head_1, head_2, ..., head_h)W^O</script><script type="math/tex; mode=display">
head_i=Attention(QW_i^Q, KW_i^K, VW_i^V)</script><script type="math/tex; mode=display">
W_i^Q, W_i^K \in R^{d_{model}\times d_{k}}, W_i^Q \in R^{d_{model}\times d_{v}}, W^O \in R^{hd_{v}\times d_{model}}</script></li>
</ul>
<ul>
<li><strong>Additive attention</strong>: Compute the compatibility function using a feed-forward network with a single hidden layer.</li>
</ul>
<p>Scaled dot-product attention and Multi-head attention were adopted at last to build Transformer. Authors briefly explained why they did this in the paper. But I will not focus on it in this blog post. </p>
<h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><p>Trasformer is a sequence transduction model completely based on attention machenism. There is not any sequencial structure when building transformer. Therefore, transformer can capture and make most use of the information within input sequence, and can be trained more efficiently than other sequence transduction models based on RNNs.</p>
<h2 id="Model-structure"><a href="#Model-structure" class="headerlink" title="Model structure"></a>Model structure</h2><p>Transformer is still of encoder-decoder structure, and consists of an encoder and a decoder.</p>
<p><strong>Encoder</strong> is composed of a stack of N (=6) identical encoder units. Each encoder unit contains 2 sublayers. The first sublayer is a multi-head self-attention layer. The second is a fully-connected layer. Additionally, a residual connection was applyed around each sublayer, followed by layer normalization. </p>
<img src="/2019/01/05/Transformer/2.jpg">
<p><strong>Decoder</strong> is composed of a stack of N (=6) identical decoder units. Each decoder unit contains 3 sublayers. The first sublayer is a masked multi-head self-attention layer (almost same with the self-attention layer in encoder, but prevent current position from attending to subsequent positions). The second layer is a multi-head attention over the output of corresponding encoder stack. The third layer is a fully-connected layer. And a residual connection was around  each sublayer, followed by layer normalization.  </p>
<img src="/2019/01/05/Transformer/3.jpg">
<img src="/2019/01/05/Transformer/4.jpg">
<h2 id="Attention-in-transformer"><a href="#Attention-in-transformer" class="headerlink" title="Attention in transformer"></a>Attention in transformer</h2><p>By checking the overall structure of transformer, we can find attention machenism were used in 3 different places in total. They are self-attention inside encoder, attention from encoder to decoder, and self-attention inside decoder. </p>
<p><strong>For self-attention inside encoder</strong>, Queries, Keys and Values are just the output of previous encoder unit.</p>
<p><strong>For attention from encoder to decoder</strong>, Queries are the output of previous decoder unit. Keys and Values are the output of the encoder.</p>
<p><strong>For self-attention inside decoder</strong>, Queries, Keys and Values are the output of previous decoder unit. But for each position, when computing weighted sum of values, weights corresponding with subsequent positions’ value will be masked out to <strong>prevent leftward information flow in the decoder</strong> (to preserve the auto-regressive property?). </p>
<h1 id="Something-else-to-say"><a href="#Something-else-to-say" class="headerlink" title="Something else to say"></a>Something else to say</h1><p>Attention is really a useful tool to build our models. It provides a high way for the flow of information and can capture rich dependence features in a sequence. Transformer is a good transduction model with performance and efficiency. It could be an important module when building models for other tasks. This is also what I want to talk in the next post (next post will be related with <strong>BERT</strong>) </p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/NLP/" rel="tag"># NLP</a>
          
            <a href="/tags/Attention/" rel="tag"># Attention</a>
          
            <a href="/tags/Transformer/" rel="tag"># Transformer</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/10/20/ner/" rel="next" title="Name Entity Recognition [CS544 USC]">
                <i class="fa fa-chevron-left"></i> Name Entity Recognition [CS544 USC]
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="lv-container" data-id="city" data-uid="MTAyMC8zODUzMC8xNTA1OA=="></div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          
            <img class="site-author-image" itemprop="image"
              src="/images/myavatar.jpg"
              alt="Wang Yipeng" />
          
            <p class="site-author-name" itemprop="name">Wang Yipeng</p>
            <p class="site-description motion-element" itemprop="description">Sharing of my daily work.</p>
        </div>

        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
            
              <a href="/archives/">
            
                <span class="site-state-item-count">10</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">9</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="mailto:yipengwa@usc.edu" target="_blank" title="E-Mail">
                  
                    <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Review"><span class="nav-number">1.</span> <span class="nav-text">Review</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Attention-Machenism"><span class="nav-number">2.</span> <span class="nav-text">Attention Machenism</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Transformer"><span class="nav-number">3.</span> <span class="nav-text">Transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Model-structure"><span class="nav-number">3.1.</span> <span class="nav-text">Model structure</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Attention-in-transformer"><span class="nav-number">3.2.</span> <span class="nav-text">Attention in transformer</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Something-else-to-say"><span class="nav-number">4.</span> <span class="nav-text">Something else to say</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Wang Yipeng</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div>

  <span class="post-meta-divider">|</span>

  <div class="theme-info">Theme &mdash; <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.2</div>


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.2"></script>



  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>


  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  




	





  





  
    <script type="text/javascript">
      (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>
  








  





  

  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
