<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="Sharing of my daily work.">
<meta property="og:type" content="website">
<meta property="og:title" content="Yipeng&#39;s Blog">
<meta property="og:url" content="http://wangyp.tech/index.html">
<meta property="og:site_name" content="Yipeng&#39;s Blog">
<meta property="og:description" content="Sharing of my daily work.">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Yipeng&#39;s Blog">
<meta name="twitter:description" content="Sharing of my daily work.">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.2',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://wangyp.tech/"/>





  <title>Yipeng's Blog</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Yipeng's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Set yourself on fire</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-tags" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://wangyp.tech/2018/10/20/ner/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Wang Yipeng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/myavatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yipeng's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/20/ner/" itemprop="url">Name Entity Recognition [CS544 USC]</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-10-20T10:32:26-07:00">
                2018-10-20
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Definition of NER task: to indentify all mentions of named entities (people, organizations, locations, etc.). Such a task can be boiling down to a <strong>sequence tagging problem</strong> (given a sequence of tokens, output a sequence of corresponding labels). </p>
<p>In this post, I am going to introduce three ways to model and complete this task, and understand intrinsic relation and difference among these methods.</p>
<h1 id="Generative-V-S-discriminative-models"><a href="#Generative-V-S-discriminative-models" class="headerlink" title="Generative V.S. discriminative models"></a>Generative V.S. discriminative models</h1><p>Referenced paper: <a href="http://robotics.stanford.edu/%7Eang/papers/nips01-discriminativegenerative.pdf" target="_blank" rel="noopener">2001, Andrew Y. Ng and Michale I. Jordan</a></p>
<p>Before looking into detailed algorithms , we need to figure out two categories of models which those algorithms are based on. First, I am going to list relations and differences between generative and discrominative model directly. Then I will provide an example to help you understand this stuff intuitively.</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>Generative model</th>
<th>Discriminative model</th>
</tr>
</thead>
<tbody>
<tr>
<td>Difference</td>
<td>Model the joint probability of label <strong>y</strong> and sample <strong>x</strong>, i.e. <strong>P(x,y)</strong></td>
<td>Model the conditional probability of label <strong>y</strong> and sample <strong>x</strong></td>
</tr>
<tr>
<td>Relation</td>
<td>Can get discriminative model by using Bayes formular from generative model</td>
<td>Not the opposite</td>
</tr>
<tr>
<td>Feature</td>
<td><strong>1)</strong> Contain more infornation inside the model than discriminative model; <strong>2)</strong> Can be used in unsupervised learning; <strong>3)</strong> Is competible with online learning method and missing data situation;</td>
<td><strong>1)</strong> Need labeled data and suitabe for (semi-)supervised learning; <strong>2)</strong> Easier to train and can get better performance in classification tasks in general. <strong>3)</strong> It is free to design features; <strong>4)</strong> Cannot give a full picture of the whole training data; <strong>5)</strong> Black box</td>
</tr>
<tr>
<td>Example</td>
<td>Naive bayes; HMMs; Markov random fields, etc.</td>
<td>Logistic regression; SVMs; Neural networks; Conditional random fields, etc.</td>
</tr>
</tbody>
</table>
</div>
<p>Example:</p>
<p>Suppose we have following training data <code>(1,0), (1,0), (2,0), (2,1)</code>.</p>
<p>Generative model learns joint probability distribution $P(x,y)$ form it:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>y = 0</th>
<th>y = 1</th>
</tr>
</thead>
<tbody>
<tr>
<td>x = 1</td>
<td>1/2</td>
<td>0</td>
</tr>
<tr>
<td>x = 2</td>
<td>1/4</td>
<td>1/4</td>
</tr>
</tbody>
</table>
</div>
<p>Discriminative model learns conditional probability distribution $P(y|x)$ from it: </p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>y = 0</th>
<th>y = 1</th>
</tr>
</thead>
<tbody>
<tr>
<td>x = 1</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>x = 2</td>
<td>1/2</td>
<td>1/2</td>
</tr>
</tbody>
</table>
</div>
<p>The distribution $P(y|x)$ is the natural distribution for classifying a given sample $x$ into a class $y$, which is why algorithms that model this directly are called discriminative algorithms. Generative algorithms model $P(x,y)$, which can be tranformed into $P(y|x)$ by applying Bayes rule and then used for classification. However, the distribution $P(x,y)$ can also be used for other purposes because it provides a full picture of the whole training set. For example you could use $p(x,y)$ to generate likely $(x,y)$ pairs.</p>
<h1 id="Independent-Classifiers"><a href="#Independent-Classifiers" class="headerlink" title="Independent Classifiers"></a>Independent Classifiers</h1><p>$t$ is the tag sequence to be predicted. $w$ is given word sequence. We want to model $P(t|w)$ and do prediction based on this ($argmax_{\mathbf{t}}P(\mathbf{t}|\mathbf{w})$). Same do other following models.</p>
<script type="math/tex; mode=display">
P(\mathbf{t}|\mathbf{w})=\Pi_{i}P(t_i|\mathbf{w})</script><h1 id="Hidden-Markove-models-HMMs"><a href="#Hidden-Markove-models-HMMs" class="headerlink" title="Hidden Markove models (HMMs)"></a>Hidden Markove models (HMMs)</h1><script type="math/tex; mode=display">
P(\mathbf{t}|\mathbf{w})=\Pi_{i}P(w_i|t_i)P(t_i|t_{i-1})</script><img src="/2018/10/20/ner/hmm.jpg">
<p>A brief deriving process is showed below:</p>
<script type="math/tex; mode=display">
P(\mathbf{t}|\mathbf{w})=P(\mathbf{w},\mathbf{t})/P(\mathbf{w})\propto P(\mathbf{w},\mathbf{t})=P(\mathbf{w}|\mathbf{t})P(\mathbf{t})=\Pi_{i}P(w_i|t_i)P(t_i|t_{i-1})</script><p>It is easy to know what we model here is the joint probability distribution $P(w,t)$. Therefore, <strong>HMM is a generative model</strong>. </p>
<p>The last step is because 2 assuptions. And the algorithm to find optimal $t$ to maximize $P(t|w)$ is <strong>Viterbi algorithm</strong>. Detailed infomation of the 2 assuptions and implement of the algorithm can be found <a href="http://wangyp.tech/2018/09/08/POS%20tag%20&amp;%20HMM/">here</a>.</p>
<h1 id="Maximunm-entropy-Markov-models-MEMMs"><a href="#Maximunm-entropy-Markov-models-MEMMs" class="headerlink" title="Maximunm entropy Markov models (MEMMs)"></a>Maximunm entropy Markov models (MEMMs)</h1><script type="math/tex; mode=display">
P(\mathbf{t}|\mathbf{w})=\Pi_{i}P(t_i|w_i,t_{i-1})</script><img src="/2018/10/20/ner/memm.jpg"> 
<p>What we model here is conditional probability distribution $P(t|w)$. Therefore, MEMM is a discriminative model.</p>
<p>Now, lets parameterize it:</p>
<script type="math/tex; mode=display">
P(t_i|w_i,t_{i-1})=softmax(\mathbf{S})_{i}</script><script type="math/tex; mode=display">
\mathbf{S}_i=s(t_i,t_{i-1},w_i)</script><script type="math/tex; mode=display">
s(t_i,t_{i-1},w_i)=\sum_{k}\lambda_kf_k(t_i,t_{i-1},w_i)</script><p>Where:</p>
<ul>
<li>$f_k$ is <strong>feature function</strong> (can be disigned freely, like emission-like feature, transition-like feature, etc.).</li>
<li>$\lambda$ is <strong>weight</strong> associated with each specific type of feature. </li>
<li>$s()$ is <strong>score function</strong>.</li>
<li>S is the vector consisting of all scores.</li>
<li>Nomalize S using softmax function then get the probability we want.</li>
</ul>
<p><strong>More about MEMMS</strong></p>
<p>Emission features can go across multiple observations. It is especially usefule for shallow parsing and NER tasks.</p>
<script type="math/tex; mode=display">
s(t_i,t_{i-1},w_i)=\sum_k\lambda_kf_k(t_i,t_{i-1},\mathbf{w})</script><img src="/2018/10/20/ner/memm2.jpg">
<p>[Does it has any relation with attention mechanism?]</p>
<h2 id="Conditional-Random-Field-CRF"><a href="#Conditional-Random-Field-CRF" class="headerlink" title="Conditional Random Field (CRF)"></a>Conditional Random Field (CRF)</h2><p>In MEMMs, we only consider the dependency among <script type="math/tex">t_{i}</script>,  <script type="math/tex">t_{i-1}</script>,  <script type="math/tex">\mathbf{w}</script>. But in some cases, like <strong>Label biased problem</strong>. Therefore, we want to model global dependency. This is the intuition of CRF.</p>
<script type="math/tex; mode=display">
P(\mathbf{t}|\mathbf{w})=\frac{S(\mathbf{t},\mathbf{w})}{\sum_{\mathbf{t}^\prime}\exp S(\mathbf{t}^{\prime},\mathbf{w})}</script><script type="math/tex; mode=display">
S(\mathbf{t},\mathbf{w})=\sum_i(\sum_k\lambda_kf_k(t_i,\mathbf{w}))+\sum_l\gamma_lg_l(t_i,t_{i-1},\mathbf{w}))</script><img src="/2018/10/20/ner/crf.jpg">
<p>Normalize globally, and score entire sequence directly.</p>
<p>What we model here is conditional probability distribution $P(t|w)$. Therefore, CRF is a discriminative model.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-tags" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://wangyp.tech/2018/09/12/Syntax Tree And CKY Parser/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Wang Yipeng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/myavatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yipeng's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/12/Syntax Tree And CKY Parser/" itemprop="url">Syntax Tree And CKY Parser [CS544 USC]</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-09-12T23:24:56-07:00">
                2018-09-12
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>The characteristics of the words and their relative location to each other allow us to generate a interpretation of a sentence even though we don’t really understand the sentence. It implies that the grammer structure of a sentence can help us understanding  it in some degree. Therefore, parsing a sentence into a syntax tree will be a very valuable work.</p>
<h1 id="CFG-and-Syntax-Tree"><a href="#CFG-and-Syntax-Tree" class="headerlink" title="CFG and Syntax Tree"></a>CFG and Syntax Tree</h1><p>Context-free grammer (CFG) is a certain type of grammer. A context-free grammar G is defined by the 4-tuple:</p>
<p><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2543ae859dd620f858299855868ae6c2ed066323" alt="G=(V,\Sigma ,R,S)"></p>
<p>V is a finite set of nonterminal characters, usually to be PoS tags.  Sigma is a finite set of terminals, the actural content of sentence. R is a finite set of rewrite rules.</p>
<script type="math/tex; mode=display">
V=\{S,NP,PP,VP,Vst,Det,V,N...\}</script><script type="math/tex; mode=display">
\Sigma=\{time,flies,like,an,arrow\}</script><script type="math/tex; mode=display">
R=\{S\rightarrow(NP,VP),S\rightarrow(Vst,NP),VP\rightarrow(V,NP)...\}</script><p>S is the start variable (or start symbol), used to represent the whole sentence (or program). It must be an element of V.</p>
<p>Based on G, we can build a syntax tree of a given sentence (sequence of words). A example syntax tree looks like:</p>
<img src="/2018/09/12/Syntax%20Tree%20And%20CKY%20Parser/syntax_tree.jpg">
<h1 id="CKY-Parser-CKY-Algorithm"><a href="#CKY-Parser-CKY-Algorithm" class="headerlink" title="CKY Parser (CKY Algorithm)"></a>CKY Parser (CKY Algorithm)</h1><h2 id="How-CKY-wroks"><a href="#How-CKY-wroks" class="headerlink" title="How CKY wroks"></a>How CKY wroks</h2><p>How to parse a sentence into a syntax tree (or syntax trees)? Yes, CKY parser (algorithm) is the tool we need. CKY algorithm is dynamic programming algorithm. </p>
<p>Let <em>s</em> denote the sentence (word sequence), <em>phrase(X, i, j)</em> denote the PoS tag for subsequence from xi to xj with <em>X</em> as root node:</p>
<script type="math/tex; mode=display">
s=(x_1,x_2,x_3,...,x_n),\ x_i\in\Sigma</script><script type="math/tex; mode=display">
phrase(X,i,j),\ X\in V,\ 1\leq i,j\leq n</script><p>The recursive formular is:</p>
<script type="math/tex; mode=display">
phrase(X,i,j)=rewrite(X,Y,Z),\ phrase(Y,i,k),\ phrase(Z,k+1,j)</script><script type="math/tex; mode=display">
X\rightarrow(Y,Z)\in R</script><script type="math/tex; mode=display">
k\in\{i,i+1,...,j-1\}</script><p>The initial state is:</p>
<script type="math/tex; mode=display">
phrase(X,i,i)=rewrite(X,W),\ word(W,i,i)</script><script type="math/tex; mode=display">
W\in\Sigma</script><p>If we traceback from the top, <em>phrase(X, 1, n)</em>, we can get all  possible syntax of the given sentence under the given CFG. Please refer to the CSCI544-slides for the pseudocode. My notation may be a little different from the slides for I am trying to make it more understandable &gt;_&lt;’’</p>
<h1 id="Probabilistic-CKY-Parser-Vanilla-PCFGs"><a href="#Probabilistic-CKY-Parser-Vanilla-PCFGs" class="headerlink" title="Probabilistic CKY Parser (Vanilla PCFGs)"></a>Probabilistic CKY Parser (Vanilla PCFGs)</h1><h2 id="How-PCKY-works"><a href="#How-PCKY-works" class="headerlink" title="How PCKY works"></a>How PCKY works</h2><p>For a given sentence and a given CFG, there meybe (actually usually) more than one possible syntax tree. It will cause ambiguity. If we assign a ‘weight’ to every rule, we can calculate the weight of every possible tree and pick one from them withour ambiguity.</p>
<p>Then it comes a question, how to define a ‘weight’ and how to pick it. If we define the weight of a rule a its possibility to occur, then all possible syntax tree will have a distribution. Based on this definition, we have a modified algorithm, probabilistic CKY parser (or PCFG).</p>
<p>For given CFG, define T as all possible syntax tree, we are going to get p(t), the distribution over T and choose the syntax tree with the largest probability. </p>
<p>Let <em>s</em> denote the sentence (word sequence), <em>T(X, i, j)</em> denote all possible syntax trees for subsequence from xi to xj with <em>X</em> as root node. Then define</p>
<script type="math/tex; mode=display">
\pi(X,i,j)=\max_{t\in T(X,i,j)}p(t)</script><p>Initial state:</p>
<script type="math/tex; mode=display">
\pi(X,i,i)=
\left\{
    \begin{aligned}
    &p(X\rightarrow x_i)&if\ X\rightarrow x_i\in R\\
    &0&otherwise 
    \end{aligned}
\right.</script><script type="math/tex; mode=display">
p(X\rightarrow x_i)=\frac{count(X\rightarrow x_i)}{count(X)}</script><p>The recursive formular is:</p>
<script type="math/tex; mode=display">
\pi(X,i,j)=\max_{X\rightarrow(Y,Z)\in R,\ i\le k<j}p(X\rightarrow(Y,Z))\times\pi(Y,i,k)\times\pi(Z,k+1,j)</script><script type="math/tex; mode=display">
p(X\rightarrow(Y,Z))=\frac{count(X\rightarrow(Y,Z))}{count(X)}</script><p>At last, the syntax tree we want is:</p>
<script type="math/tex; mode=display">
T_{best}=\mathop{\arg\max}_{t\in T(X,1,n),\ x\in V}p(t)</script><h2 id="Applicable-tasks"><a href="#Applicable-tasks" class="headerlink" title="Applicable tasks"></a>Applicable tasks</h2><ul>
<li>Given a sentence and a CFG, to find the best parsing (syntax tree) of the sentence.</li>
<li>Given a CFG, to get the probability of a sentence to be generated.</li>
</ul>
<p>In task2, we need to add up probabilities of all possible parsing result of the sentence.</p>
<h2 id="Generalizing"><a href="#Generalizing" class="headerlink" title="Generalizing"></a>Generalizing</h2><p>If we apply a -log() on both sides of the recursive formular, we get <strong>weighted CKY parser</strong>. And what we want becomes the syntax tree with the lowest weight (-log cost). PCKY is actually a special case of WCKY.</p>
<p>WCKY can be generalized into <strong>semiring-weighted CKY</strong> (the most general algorithm). Just use semiring operation (should obey <strong>semiring axioms</strong>) to replace original specific operation.  Deatails could be found in CSCI544-slides. </p>
<h2 id="Drawback-and-improvement"><a href="#Drawback-and-improvement" class="headerlink" title="Drawback and improvement"></a>Drawback and improvement</h2><p>The information of word relations does not contribute much to the building of the whole syntax tree of a sentence. <strong><em>(I really hope u guys could give some better explanation for this)</em></strong></p>
<p>Instead of using PoS tags only (elements in V), we use tuple <strong>(head word, PoS tag)</strong> for each nonterminal character.</p>
<p>original:</p>
<script type="math/tex; mode=display">
S\rightarrow NP,\ VP</script><p>After <strong>lexicalized</strong>:</p>
<script type="math/tex; mode=display">
S(loves,VB)\rightarrow NP(John,NNP),\ VP(loves,VB)</script><p><strong>References:</strong></p>
<p><a href="https://github.com/isi-nlp/csci_544_fa_18_slides/blob/master/pcfg_0914.pptx" target="_blank" rel="noopener">https://github.com/isi-nlp/csci_544_fa_18_slides/blob/master/pcfg_0914.pptx</a></p>
<p><a href="https://blog.csdn.net/qq_37171771/article/details/79342819" target="_blank" rel="noopener">https://blog.csdn.net/qq_37171771/article/details/79342819)</a></p>
<p><a href="https://en.wikipedia.org/wiki/Probabilistic_context-free_grammar" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Probabilistic_context-free_grammar</a></p>
<p><a href="http://ccl.pku.edu.cn/doubtfire/Course/Computational%20Linguistics/contents/CYK_parsing.pdf" target="_blank" rel="noopener">http://ccl.pku.edu.cn/doubtfire/Course/Computational%20Linguistics/contents/CYK_parsing.pdf</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-tags" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://wangyp.tech/2018/09/08/POS tag & HMM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Wang Yipeng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/myavatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yipeng's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/08/POS tag & HMM/" itemprop="url">POS tag & HMM [CS544 USC]</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-09-08T09:33:13-07:00">
                2018-09-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="POS-Tag"><a href="#POS-Tag" class="headerlink" title="POS Tag"></a>POS Tag</h1><p>POS is short for “part of speech” which also means <strong>syntactic categories</strong>. POS tagging is the first step toward full syntactic analysis and it is often good features for other tasks.</p>
<p>The tag of a word depends on the word itself and the tags of surrounding words. Such a situation can be descripbed by HMM probabilistically.</p>
<ul>
<li>Open class words: It is also called content words. It could be nouns, verbs, adjectives or adverbs usually. There is no limit to what they are and can be added all the times.</li>
<li>Closed class words: It is also called function words. It could be pronouns, determiners, prepositions or connectives usually. There are a limit number of these words. </li>
</ul>
<h1 id="Different-Sets-of-POS-Tags"><a href="#Different-Sets-of-POS-Tags" class="headerlink" title="Different Sets of POS Tags"></a>Different Sets of POS Tags</h1><p>There are many different sets of POS tags based on different linguistic and practical considerations. We can choose to use one of them according to our specific task. </p>
<p>Commonly used tag sets for English usually have 40 to 100 tag types. The <strong><a href="https://www.clips.uantwerpen.be/pages/mbsp-tags" target="_blank" rel="noopener">Penn Treebank</a></strong> has 45 tags.</p>
<h1 id="Modeling-POS-Tagging-with-HMM-1"><a href="#Modeling-POS-Tagging-with-HMM-1" class="headerlink" title="Modeling POS Tagging with HMM[1]"></a>Modeling POS Tagging with HMM[1]</h1><p>What we are interested in is P(W|T) and P(T) where W and T are sequences of length N.</p>
<ul>
<li>Assumption 1: Each tag is conditioned only on the previous tag. This is so-called <strong>Markov Property</strong></li>
<li>Assumption 2: Each word is conditioned only on its tag.</li>
</ul>
<p>Based on these two assumptions, we can use HMM to describe the POS tagging process. </p>
<script type="math/tex; mode=display">
argmax_{T}P(T|W)=argmax_{T}P(W|T)P(T)</script><p>To solve this optimization problem, we can use <strong>Greedy Algorithm</strong>, i.e. at each time step, compute the possibility under each possible tag given the previous tag and choose the best one. It is easy to know the time complexity of greedy algorithm is <strong>O(|T|*N)</strong>.</p>
<p>Greedy algorithm’s result is suboptimal. To get the optimal tag sequence we need <strong>Viterbi Algorithm[2]</strong>. If we use T to denote tag sequence, W to denote word sequence, and v[T, W] to denote be the probability of the best T-W pair, the algorithm can be described with formulas below: </p>
<script type="math/tex; mode=display">
T_{1:i}=[t_1,t_2,...,t_i]</script><script type="math/tex; mode=display">
W_{1:i}=[w_1,w_2,...,w_i]</script><script type="math/tex; mode=display">
v[T_{1:i},W_{1:i}]=max_{t_{i}}v[T_{1:i-1},W_{1:i-1}]\times P(t_{i}|t_{i-1})\times P(W_{i}|t_{i})</script><p><strong>[1] Hidden Markov Model (HMM)</strong> is a statistical <a href="https://en.wikipedia.org/wiki/Markov_model" target="_blank" rel="noopener">Markov model</a> in which the system being modeled is assumed to be a <a href="https://en.wikipedia.org/wiki/Markov_process" target="_blank" rel="noopener">Markov process</a> with unobserved (i.e. <em>hidden</em>) states.</p>
<p>In simpler Markov models (like a <a href="https://en.wikipedia.org/wiki/Markov_chain" target="_blank" rel="noopener">Markov chain</a>), the state is directly visible to the observer, and therefore the state transition probabilities are the only parameters, while in the hidden Markov model, <strong><em>the state (i.e. POS tag in our discussion)</em></strong> is not directly visible, but the <strong><em>output (i.e. Word)</em></strong>, dependent on the state, is visible. Each state has a probability distribution over the possible output tokens. Therefore, the sequence of tokens generated by an HMM gives some information about the sequence of states.</p>
<p><strong>[2] Viterbi algorithm</strong> is a <a href="https://en.wikipedia.org/wiki/Dynamic_programming" target="_blank" rel="noopener">dynamic programming</a> algorithm for finding the most likely sequence of hidden states—called the <strong>Viterbi path</strong>—that results in a sequence of observed events, especially in the context of Markov information sources and hidden Markov models.</p>
<p>Intuition: the best path of length i ending in state must include the best path of length i-1 to the previous state.</p>
<p>Baisc steps: Best path of length i-1 to each state —&gt; Extend each path to state t and find the best one —&gt; Best path of length i in state t. </p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-tags" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://wangyp.tech/2018/07/29/What gonna happen next/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Wang Yipeng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/myavatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yipeng's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/29/What gonna happen next/" itemprop="url">What gonna happen next?</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-07-29T02:16:45-07:00">
                2018-07-29
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Event-Prediction-Based-on-“Event-Embedding”"><a href="#Event-Prediction-Based-on-“Event-Embedding”" class="headerlink" title="Event Prediction Based on “Event Embedding”"></a>Event Prediction Based on “Event Embedding”</h1><p><a href="https://aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11995/12015" target="_blank" rel="noopener">Original paper by Mark Granroth-Wilding and Stephen Clark: What Happens Next? Event Prediction Using a Compositional Neural Network Model</a></p>
<h1 id="Recall"><a href="#Recall" class="headerlink" title="Recall"></a>Recall</h1><p>Let’s recall the original work of Chambers and Jurafsky in 2008. In that <a href="https://nlp.stanford.edu/pubs/narrative-chains08.pdf" target="_blank" rel="noopener">paper</a>, an <strong><em>event</em></strong> is defined as a pair of the predicate <strong><em>(verb)</em></strong> and  the grammatical dependency <strong><em>(subj, obj, or iobj)</em></strong>. So an event is expressed in a form of <strong><em>predicate-GR</em></strong>.</p>
<p>C&amp;J looks only at the co-occurrences of predicate-GRs. The prediction is totally based on statistical analysis. But there are still some obvious problems:</p>
<ul>
<li>A lot of information is lost when we transfer an entity of an event into GR. The meaning of an event could be drastically changed by its entities. <strong><em>Eg1. (perform, play<subj>) VS. (perform surgery<subj>); Eg2. (go, on holiday<subj>) VS. (go, to church<subj>)</subj></subj></subj></subj></em></strong>  </li>
<li>The model are not able to assign scores to unobserved pairs. And only events with frequent predicates could be scored accurately.</li>
</ul>
<h1 id="Insight"><a href="#Insight" class="headerlink" title="Insight"></a>Insight</h1><p>Inspired by the paper word2vec (2013), Two authors (Mark &amp; Stephen) use contininuous vector embeddings of words and events to replace the original definition on an event (predicate-GR). Then M&amp;S train a neural model to score any input event-vector pairs.</p>
<h1 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h1><p><strong>Verb-only model</strong></p>
<p>M&amp;S represent an event using the vector <em>(MIKOLOV-VERB)</em> for its verb. Vectors of events are summed up as the description of the context (i.e the event chain we reffered in my last post). Finally, compute the cosine similarity between the context and candidate event.</p>
<script type="math/tex; mode=display">
s(c)=cosine(\sum^{n-1}_{j=0}W_{p(e_j)},W_{p(c)})</script><p><strong>Verb-argument model</strong></p>
<p>Just a little difference from the model above. An event is repersened as the sum of the vectors for its verb and each of its arguments. Other definitions or methods keep unchanged.</p>
<script type="math/tex; mode=display">
s(c)=cosine(\sum^{n-1}_{j=0}[W_{p(e_j)}+W_{a_0(e_j)}+W_{a_1(e_j)}+W_{a_2(e_j)}],W_{p(c)}+W_{a_0(c)}+W_{a_1(c)}+W_{a_2(c)})</script><p>Actually, neither of two models above are adopted a last, because the roles of verb and arguments are obvious different. It is not proper to combine them linearly as the description of an event or a context. M&amp;S apply neural composition model to implement nonlinear combination of the two kinds of vectors. Such a model is called <strong><em>EVENT-COMP</em></strong>, we will talk about it later.</p>
<p><strong>Embedding model</strong></p>
<p>In the two models above, the word-vectors we used is from word-vector set provided in the work of <a href="https://arxiv.org/pdf/1301.3781.pdf" target="_blank" rel="noopener">Mikolov et al. (2013a)</a>. These word-vectors are trained based on n-gram, center word and context. </p>
<p>Inspired by this, and based on the same idea, M&amp;S treat each event’s predicate-GR as a word and each trianing chain as a sentence, and then train a skip-gram model (with hierarchical sampling, using a window size of 5 and vector size of 300), i.e. <strong><em>WORD2VEC-PRED</em></strong>.</p>
<p>Besides the approach above, M&amp;S also treat predicate and arguments as separate words and place them in the ‘sentence’. Using the similar method, they trained another model <strong><em>WORD2VEC-PRED+ARG</em></strong>, this is also the embedding model adopted finally.</p>
<p><strong>Neural Compositional Representations</strong></p>
<p>We want to train a neural network <strong><em>EVENT-COMP</em></strong> to learn a non-linear composition of predicates and arguments into an event representation and a similarity model to determine the similarity between two vectorized events.</p>
<ul>
<li><strong>Input Layer</strong> The vectors of the predicate and arguments coresponding to an event. Empty  arguments are filled with zero vectors.</li>
<li><strong>Argument Composition Layer</strong> It consists of a stack of <a href="http://deeplearning.net/tutorial/dA.html" target="_blank" rel="noopener"><strong><em>denoising autoencoders</em></strong></a>. It produces a low-dimensional representation of the event.</li>
<li><strong>Event Composition Layer</strong> It consists of fully-connected layers. It produces a single output value <strong><em>coherence score (coh)</em></strong>. The coh represents how confident two input events are from a same chain.</li>
</ul>
<img src="/2018/07/29/What%20gonna%20happen%20next/event_comp.jpg" title="event_comp">
<p>Some training iterations are performed to update only the parameters of event composition layer; Further iterations then update all parameters. (Parameters for 2 events in input layer and argument composition layer are fixed to be same)</p>
<ul>
<li><strong>Loss function</strong><script type="math/tex; mode=display">
\frac{1}{m}\sum^{m}_{i=1}-log[p_i\times coh(e^0_i, e^1_i)+(1-p_i)\times(1-coh(e^0_i, e^1_i))]+\lambda L(\theta)</script>Pi = 1 for positive samples (2 events are from a same chain), and Pi = 0 for negative samples (2 events are from different chain). L is an l2 regularization term. The object is to minimize the loss function.</li>
</ul>
<h1 id="What-gonna-happen-next"><a href="#What-gonna-happen-next" class="headerlink" title="What gonna happen next?"></a>What gonna happen next?</h1><p>After training all the models. Given a chain of events, we can compute a <strong><em>coh</em></strong> for a candidate event and each event in the chain. Then sum up these scores. So that we can choose a candidate event with the largest total score as our prediction.</p>
<script type="math/tex; mode=display">
s(c)=\frac{1}{n}\sum^{n-1}_{j=0}coh(c, e_j)</script><h1 id="Extra"><a href="#Extra" class="headerlink" title="Extra"></a>Extra</h1><p>M&amp;S also provide a effective evaluation method to test the performance of event prediction models called <strong><em>Multiple Choice Version of the Narrative Cloze Task (MCNC)</em></strong>. I am not gonna talk about it here, because it works like what it looks like.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-tags" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://wangyp.tech/2018/07/25/NarrativeChains/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Wang Yipeng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/myavatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yipeng's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/25/NarrativeChains/" itemprop="url">Narrative Event Chains</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-07-25T16:42:29-07:00">
                2018-07-25
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><p>We can always break a narrative into some events, for example:</p>
<p>given a narrative  <em><strong>“John entered the restaurant. He sat down, and ordered a meal. He ate…”</strong></em></p>
<p>it can breaks into events looks like</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">enter =&gt; sit =&gt; order =&gt; eat</span><br></pre></td></tr></table></figure>
<p>By doing a simple statistic, we can simply abstract some valuable information of the narrative like:</p>
<p> <strong><em>Given a current event-A, how much is event-B likely to occur next?</em></strong></p>
<p>But this is not informative enough,<del>and too much of verbs barely exceed chance performance</del>!</p>
<h1 id="Insight"><a href="#Insight" class="headerlink" title="Insight"></a>Insight</h1><p>Don’t look at all verbs, just focus on those mentioning the “key actor” —  <em><strong>protagonist</strong></em></p>
<p>Instead of using verbs only, but pair verbs with their protagonist.</p>
<h1 id="Task"><a href="#Task" class="headerlink" title="Task"></a>Task</h1><p>In total, our task is to extract events that  constitute <em><strong>narrative chains</strong></em></p>
<ul>
<li><em><strong>Narrative chain</strong></em> is a <em><strong>partially ordered</strong></em> set of <em><strong>narrative events</strong></em></li>
<li><em><strong>Narrative event</strong></em> is a tuple of  <em><strong>“event” </strong></em> and <em><strong>“participants” </strong></em></li>
<li><em><strong>Partially ordering</strong></em> means some narrative event is <em><strong>before</strong></em> another one</li>
<li>In our circumstance, the “event” is represented as a <em><strong>verb</strong></em>, the “participants” is represented as a <em><strong>typed dependency</strong></em> </li>
</ul>
<p>If we use <script type="math/tex">e_i=(verb, dependency)</script> to denote a narrative event,</p>
<p>the narrative chain should be <script type="math/tex">\{e_1, e_2, ..., e_n\}</script></p>
<p>$B(e_i, e_j) = True$ if <script type="math/tex">e_i</script> occurs strictly before <script type="math/tex">e_j</script></p>
<h2 id="Stage-1-Learning-narrative-relations"><a href="#Stage-1-Learning-narrative-relations" class="headerlink" title="Stage 1 Learning narrative relations"></a>Stage 1 Learning narrative relations</h2><p><em><strong>Unsupervised cluster</strong></em></p>
<ol>
<li><p><strong>Pairwise relations between events</strong></p>
<p>Given a list of observed <em><strong>narrative events</strong></em>, we can approximate the <em><strong>point-wise mutual information (PMI)</strong></em> by:</p>
<script type="math/tex; mode=display">prob[(verb_i, X)AND(verb_j, X)]=\frac{num_X[(verb_i, X)AND(verb_j, X)]}{sum_{i,j}\{num_X[(verb_i, X)AND(verb_j, X)]\}}</script><script type="math/tex; mode=display">PMI[(verb_i, X), (verb_j, X)]=log\frac{prob[(verb_i, X) AND(verb_j, X)]}{prob[(verb_i, X)]prob[(verb_j, X)]}</script><p><em><strong>PMI</strong></em> describes: given a event (with a argument), how “unusual” to see another event even (with the same argument)</p>
</li>
<li><p><strong>Global narrative score</strong></p>
<p>Given all narrative events in a document, we can find next most likely event to occur by maximizing:</p>
<script type="math/tex; mode=display">max_{0<j<m}\sum^n_{i=0}PMI(e_i, f_j)</script><p><em><strong>n</strong></em> is the number of given events in our chain, and <em><strong>m</strong></em> is the number of events in our training corpus.</p>
<p>Such a <em><strong>global narrative score</strong></em> describes: given n events (with a argument), how “unusual” to see another event even (with the same argument)</p>
<p><strong>[Example]</strong></p>
<p>Three narrative events and the six most likely events to include in the same chain (actually, is set, because it is not ordering now)</p>
<blockquote>
<p>   <strong>Known events</strong></p>
<p>   (pleaded subj)</p>
<p>   (admits subj)</p>
<p>   (convicted obj)</p>
<p>   <strong>Likely events</strong></p>
<p>   (sentenced obj) 0.89</p>
<p>   (paroled obj) 0.76</p>
<p>   (fired obj) 0.75</p>
<p>   (indicted obj) 0.74</p>
<p>   (fined obj) 0.73</p>
<p>   (denied subj) 0.73</p>
</blockquote>
</li>
<li><p><strong>Evaluation</strong></p>
<p><em><strong>Narrative cloze</strong></em> is a sequence of narrative events in a document, from which <em><strong>one event</strong></em> has been removed, the task is to <em><strong>predict the missing (verb, typed dependency)</strong></em> </p>
<p><strong>[Example]</strong></p>
<p>Given a document and focus <em><strong>McCann</strong></em> as protagonist:</p>
<blockquote>
<p><em><strong>McCann</strong></em> <em>threw</em> two interceptions early.</p>
<p>Toledo <em>pulled</em> <strong><em>McCann</em></strong> aside and <em>told</em> <strong><em>him</em></strong> <strong><em>he’d</em></strong> <em>start</em>.</p>
<p><strong><em>McCann</em></strong> quickly <em>completed</em> his first two passes.</p>
</blockquote>
<p>We got five events:</p>
<blockquote>
<p>(threw subj)</p>
<p>(pulled obj)</p>
<p>(told obj)</p>
<p>(start subj)</p>
<p>(completed subj) </p>
</blockquote>
<p>We remove the first one <strong><em>(threw subj)</em></strong> and use the remaining four events to rank the missing event (chosen from extra <strong><em>training data</em></strong>)</p>
</li>
</ol>
<h2 id="Stage-2-Ordering-narrative-events"><a href="#Stage-2-Ordering-narrative-events" class="headerlink" title="Stage 2 Ordering narrative events"></a>Stage 2 Ordering narrative events</h2><p><em><strong>Supervised classification</strong></em></p>
<p>class 1. <strong><em>before</em></strong></p>
<p>class 2. <strong><em>other</em></strong></p>
<p>Each pair of <strong><em>narrative events</em></strong> in a document that share a coreferring argument is treated as a separate classification task.</p>
<p>We count the number of labeled <strong><em>before</em></strong> relations between each <strong><em>narrative event</em></strong>.</p>
<p>Then build a database of such a count.</p>
<p>We can measure the <strong><em>confidence</em></strong> of <strong><em>“event-X before event-Y”</em></strong> by looking up the database and compute <script type="math/tex">num(B(X, Y)=True)/num(B(Y, X)=True)</script></p>
<p><strong>[Evaluation]</strong></p>
<p>Given a set of narrative events: </p>
<ul>
<li><strong>Chain 1:</strong> These events are hand-labeled for a partial ordering </li>
<li><strong>Chain 2:</strong> These events are sorted in random order</li>
</ul>
<p>We gonna to decide which of these two chains is more coherent.</p>
<p>To to this, we need to introduce <strong><em>coherence score</em></strong>.</p>
<p><strong><em>Coherence score</em></strong> is the sum of all relations (edge in the graph) that our model agrees with. </p>
<p>Every edge is weighted by its <strong><em>confidence</em></strong></p>
<img src="/2018/07/25/NarrativeChains/example1.jpg">
<p><strong>[*Notice]</strong></p>
<p>In real practice, the mathematical definition of <em><strong>confidence</strong></em> will be more complex, but here I just simplify it to make it more understandable. Please check <a href="https://nlp.stanford.edu/pubs/narrative-chains08.pdf" target="_blank" rel="noopener">the original paper</a> for formal definition.  </p>
<h2 id="Stage-3-Discrete-narrative-event-chains"><a href="#Stage-3-Discrete-narrative-event-chains" class="headerlink" title="Stage 3 Discrete narrative event chains"></a>Stage 3 Discrete narrative event chains</h2><p>Up to now, we have got <strong><em>narrative relations</em></strong> across all possible events and their <strong><em>temporal order</em></strong> (i.e. many partially ordered narrative event chains).</p>
<p>But they are still discrete list and cannot reproduce explicit self-contained scripts.</p>
<p>However, it is totally worthwhile to construct discrete narrative chains, because it can be used to check <strong><em>whether the combination of events learning and ordering has produced script-like structures</em></strong> </p>
<p>And, to get a whole combination of events, we only need to apply a <strong><em>cluster algorithm</em></strong> over <strong><em>all events</em></strong> based on <strong><em>PMI score</em></strong> we got before, and use approach stated in stage 2 to extract “before” relation among them.</p>
<img src="/2018/07/25/NarrativeChains/example2.jpg">
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>This the very basic and important work in the area of NLP. It provides a new idea for “language generation”, “question answering”, “frame induction” and many other areas.</p>
<p><a href="https://nlp.stanford.edu/pubs/narrative-chains08.pdf" target="_blank" rel="noopener">*original paper: “Unsupervised Learning of Narrative Event Chains”</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          
            <img class="site-author-image" itemprop="image"
              src="/images/myavatar.jpg"
              alt="Wang Yipeng" />
          
            <p class="site-author-name" itemprop="name">Wang Yipeng</p>
            <p class="site-description motion-element" itemprop="description">Sharing of my daily work.</p>
        </div>

        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
            
              <a href="/archives/">
            
                <span class="site-state-item-count">9</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">7</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="mailto:yipengwa@usc.edu" target="_blank" title="E-Mail">
                  
                    <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Wang Yipeng</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div>

  <span class="post-meta-divider">|</span>

  <div class="theme-info">Theme &mdash; <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.2</div>


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.2"></script>



  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>


  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  




	





  





  








  





  

  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
