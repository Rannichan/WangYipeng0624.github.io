<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="Sharing of my daily work.">
<meta property="og:type" content="website">
<meta property="og:title" content="Yipeng&#39;s Blog">
<meta property="og:url" content="http://wangyp.tech/index.html">
<meta property="og:site_name" content="Yipeng&#39;s Blog">
<meta property="og:description" content="Sharing of my daily work.">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Yipeng&#39;s Blog">
<meta name="twitter:description" content="Sharing of my daily work.">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.2',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://wangyp.tech/"/>





  <title>Yipeng's Blog</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Yipeng's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Set yourself on fire</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-tags" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://wangyp.tech/2019/01/05/Transformer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Wang Yipeng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/myavatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yipeng's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/05/Transformer/" itemprop="url">Attention and Transformer</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-01-05T21:16:24-08:00">
                2019-01-05
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Today, I am going to introduce a very interesting paper from Google. You can find the original paper “Attention is All You Need” on the <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">website</a>. In this paper, genius aithors propose new sequence transduction model totally based on attention, called <strong>Transformer</strong>. This model outperforms many previous model based on RNNs, CNNs and shows <strong>attention machenism</strong>‘s importance in modeling sequence.  </p>
<h1 id="Review"><a href="#Review" class="headerlink" title="Review"></a>Review</h1><p><em>Keyword : RNN, LSTM, GRU, Long-term dependency, Parallel Computing.</em></p>
<p>Before talking about attention and transformer, I would first have look of RNNs, LSTMs, GRUs and what they are good at and what not. RNN (recurrent neural network) was proposed to model sequence input. It can capture the relation between current input and the previous input. It is a kind of imitation of the nature that human form a sentence from left to right (jsut an assumption, how human generate a sentence must be more complicated). However, RNN still has drawbacks. Because of the recurrent structure of RNN, the gradient (when we using gradient descent to train an RNN) is easy to vanish or explode after several time steps (recurrences). This is what LSTMs and GRUs want to solve. Upon the original version of RNN, They introduce a structure called <strong>“GATE”</strong> to control the “forget” (of previous input) and “remember” (of current input). Gradient is not that easy to vanish or explode when traning LSTMs and GRUs. Therefore, LSTMs and GRUs can capture long-term dependency and work better than orginal RNN when modeling long sequences. But LSTMs and GRUs are still not perfect. In classic encoder-decoder models, people like to use LSTMs and GRUs to encode input sequence into a fixed-length vector. However, It is very difficult for LSTMs or GRUs to capture enough information of a sequence. </p>
<blockquote>
<p>A potential issue with this encoder–decoder approach is that a neural network needs to be able to<br>compress all the necessary information of a source sentence into a fixed-length vector. This may<br>make it difficult for the neural network to cope with long sentences, especially those that are longer<br>than the sentences in the training corpus.    </p>
<p><a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">Dzmitry Bahdanau, et al. Neural machine translation by jointly learning to align and translate, 2015</a> </p>
</blockquote>
<p><strong>Attention machenism allow decoder to directly look into source sequence, thereby solving the problem of no enough information.</strong></p>
<p>Another disadvantage of RNN (including LSTM, GRU) is its sequential nature preclude parallelization within training. Same case of encoder-decoder model, if we want to make the training parallelizable but don’t build encoder and decoder using RNNs, what we could use? A powerful weapon is Self-Attention. <strong>Self-Attention can capture inter-denpendncy within a sequence, meanwhile, most computing during training is parallelizable.</strong> </p>
<h1 id="Attention-Machenism"><a href="#Attention-Machenism" class="headerlink" title="Attention Machenism"></a>Attention Machenism</h1><p>In general, </p>
<blockquote>
<p>An attention function can be described as mapping a <strong>query</strong> and a set of <strong>key-value pairs</strong> to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a <strong>compatibility function</strong> of the<br>query with the corresponding key.</p>
<p><a href="http://papers.nips.cc/paper/7181-attention-is-all-you-need" target="_blank" rel="noopener">Vaswani, Ashish, et al. “Attention is all you need.” <em>Advances in Neural Information Processing Systems</em>. 2017.</a></p>
</blockquote>
<p>In the case of encoder-decoder model, output of each decoder unit is a query, and output of each encoder unit is a key and corresponding value (here key and corresponding are exactly the same thing). The attention distribution over the encoder outputs describes how an output of decoder unit compatible with each part of encoder’s outputs. Then decoder could directly gather information from encoder’s outputs by computing weighted sum of them, where the weight comes from the attention distribution.</p>
<img src="/2019/01/05/Transformer/1.jpg">
<p>If we want to apply attention machenism in our models, we need to determine <strong>WHERE</strong> and <strong>HOW</strong>. WHERE means ‘from what queries to what keys’ and we are going to talk about it in the next section. HOW means ‘how to compute compatibility between queries and keys’. And this paper mentioned 4 ways:</p>
<ul>
<li><p><strong>Dot-product attention</strong>: Simply compute dot products of a query with all keys, and apply a softmax function to obtain weights on the values.</p>
</li>
<li><p><strong>Scaled dot-product attention</strong>: Queries and keys are of dimension $d_k$. First compute dot products of a query with all keys, and devide each by $\sqrt{d_k}$. Then apply a softmax function to obtain weights on the values.</p>
<script type="math/tex; mode=display">
Attention(Q,K,V)=softmax(QK^T/\sqrt{d_k})V</script></li>
<li><p><strong>Multi-head attention</strong>: Use several learned projection layer to project a set of queries, keys, values into different subspaces. Then apply Scaled dot-product attention on the set of queries, keys, values in each different subspace. Concatenate all attention outputs in different subspaces. At last  use a projection layer to project the concatenation result into the dimension we want.</p>
<script type="math/tex; mode=display">
MultiHead(Q,K,V) = Concat(head_1, head_2, ..., head_h)W^O</script><script type="math/tex; mode=display">
head_i=Attention(QW_i^Q, KW_i^K, VW_i^V)</script><script type="math/tex; mode=display">
W_i^Q, W_i^K \in R^{d_{model}\times d_{k}}, W_i^Q \in R^{d_{model}\times d_{v}}, W^O \in R^{hd_{v}\times d_{model}}</script></li>
</ul>
<ul>
<li><strong>Additive attention</strong>: Compute the compatibility function using a feed-forward network with a single hidden layer.</li>
</ul>
<p>Scaled dot-product attention and Multi-head attention were adopted at last to build Transformer. Authors briefly explained why they did this in the paper. But I will not focus on it in this blog post. </p>
<h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><p>Trasformer is a sequence transduction model completely based on attention machenism. There is not any sequencial structure when building transformer. Therefore, transformer can capture and make most use of the information within input sequence, and can be trained more efficiently than other sequence transduction models based on RNNs.</p>
<h2 id="Model-structure"><a href="#Model-structure" class="headerlink" title="Model structure"></a>Model structure</h2><p>Transformer is still of encoder-decoder structure, and consists of an encoder and a decoder.</p>
<p><strong>Encoder</strong> is composed of a stack of N (=6) identical encoder units. Each encoder unit contains 2 sublayers. The first sublayer is a multi-head self-attention layer. The second is a fully-connected layer. Additionally, a residual connection was applyed around each sublayer, followed by layer normalization. </p>
<img src="/2019/01/05/Transformer/2.jpg">
<p><strong>Decoder</strong> is composed of a stack of N (=6) identical decoder units. Each decoder unit contains 3 sublayers. The first sublayer is a masked multi-head self-attention layer (almost same with the self-attention layer in encoder, but prevent current position from attending to subsequent positions). The second layer is a multi-head attention over the output of corresponding encoder stack. The third layer is a fully-connected layer. And a residual connection was around  each sublayer, followed by layer normalization.  </p>
<img src="/2019/01/05/Transformer/3.jpg">
<img src="/2019/01/05/Transformer/4.jpg">
<h2 id="Attention-in-transformer"><a href="#Attention-in-transformer" class="headerlink" title="Attention in transformer"></a>Attention in transformer</h2><p>By checking the overall structure of transformer, we can find attention machenism were used in 3 different places in total. They are self-attention inside encoder, attention from encoder to decoder, and self-attention inside decoder. </p>
<p><strong>For self-attention inside encoder</strong>, Queries, Keys and Values are just the output of previous encoder unit.</p>
<p><strong>For attention from encoder to decoder</strong>, Queries are the output of previous decoder unit. Keys and Values are the output of the encoder.</p>
<p><strong>For self-attention inside decoder</strong>, Queries, Keys and Values are the output of previous decoder unit. But for each position, when computing weighted sum of values, weights corresponding with subsequent positions’ value will be masked out to <strong>prevent leftward information flow in the decoder</strong> (to preserve the auto-regressive property?). </p>
<h1 id="Something-else-to-say"><a href="#Something-else-to-say" class="headerlink" title="Something else to say"></a>Something else to say</h1><p>Attention is really a useful tool to build our models. It provides a high way for the flow of information and can capture rich dependence features in a sequence. Transformer is a good transduction model with performance and efficiency. It could be an important module when building models for other tasks. This is also what I want to talk in the next post (next post will be related with <strong>BERT</strong>) </p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-tags" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://wangyp.tech/2018/10/20/ner/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Wang Yipeng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/myavatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yipeng's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/20/ner/" itemprop="url">Name Entity Recognition [CS544 USC]</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-10-20T10:32:26-07:00">
                2018-10-20
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Definition of NER task: to indentify all mentions of named entities (people, organizations, locations, etc.). Such a task can be boiling down to a <strong>sequence tagging problem</strong> (given a sequence of tokens, output a sequence of corresponding labels). </p>
<p>In this post, I am going to introduce three ways to model and complete this task, and understand intrinsic relation and difference among these methods.</p>
<h1 id="Generative-V-S-discriminative-models"><a href="#Generative-V-S-discriminative-models" class="headerlink" title="Generative V.S. discriminative models"></a>Generative V.S. discriminative models</h1><p>Referenced paper: <a href="http://robotics.stanford.edu/%7Eang/papers/nips01-discriminativegenerative.pdf" target="_blank" rel="noopener">2001, Andrew Y. Ng and Michale I. Jordan</a></p>
<p>Before looking into detailed algorithms , we need to figure out two categories of models which those algorithms are based on. First, I am going to list relations and differences between generative and discrominative model directly. Then I will provide an example to help you understand this stuff intuitively.</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>Generative model</th>
<th>Discriminative model</th>
</tr>
</thead>
<tbody>
<tr>
<td>Difference</td>
<td>Model the joint probability of label <strong>y</strong> and sample <strong>x</strong>, i.e. <strong>P(x,y)</strong></td>
<td>Model the conditional probability of label <strong>y</strong> and sample <strong>x</strong></td>
</tr>
<tr>
<td>Relation</td>
<td>Can get discriminative model by using Bayes formular from generative model</td>
<td>Not the opposite</td>
</tr>
<tr>
<td>Feature</td>
<td><strong>1)</strong> Contain more infornation inside the model than discriminative model; <strong>2)</strong> Can be used in unsupervised learning; <strong>3)</strong> Is competible with online learning method and missing data situation;</td>
<td><strong>1)</strong> Need labeled data and suitabe for (semi-)supervised learning; <strong>2)</strong> Easier to train and can get better performance in classification tasks in general. <strong>3)</strong> It is free to design features; <strong>4)</strong> Cannot give a full picture of the whole training data; <strong>5)</strong> Black box</td>
</tr>
<tr>
<td>Example</td>
<td>Naive bayes; HMMs; Markov random fields, etc.</td>
<td>Logistic regression; SVMs; Neural networks; Conditional random fields, etc.</td>
</tr>
</tbody>
</table>
</div>
<p>Example:</p>
<p>Suppose we have following training data <code>(1,0), (1,0), (2,0), (2,1)</code>.</p>
<p>Generative model learns joint probability distribution $P(x,y)$ form it:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>y = 0</th>
<th>y = 1</th>
</tr>
</thead>
<tbody>
<tr>
<td>x = 1</td>
<td>1/2</td>
<td>0</td>
</tr>
<tr>
<td>x = 2</td>
<td>1/4</td>
<td>1/4</td>
</tr>
</tbody>
</table>
</div>
<p>Discriminative model learns conditional probability distribution $P(y|x)$ from it: </p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>y = 0</th>
<th>y = 1</th>
</tr>
</thead>
<tbody>
<tr>
<td>x = 1</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>x = 2</td>
<td>1/2</td>
<td>1/2</td>
</tr>
</tbody>
</table>
</div>
<p>The distribution $P(y|x)$ is the natural distribution for classifying a given sample $x$ into a class $y$, which is why algorithms that model this directly are called discriminative algorithms. Generative algorithms model $P(x,y)$, which can be tranformed into $P(y|x)$ by applying Bayes rule and then used for classification. However, the distribution $P(x,y)$ can also be used for other purposes because it provides a full picture of the whole training set. For example you could use $p(x,y)$ to generate likely $(x,y)$ pairs.</p>
<h1 id="Independent-Classifiers"><a href="#Independent-Classifiers" class="headerlink" title="Independent Classifiers"></a>Independent Classifiers</h1><p>$t$ is the tag sequence to be predicted. $w$ is given word sequence. We want to model $P(t|w)$ and do prediction based on this ($argmax_{\mathbf{t}}P(\mathbf{t}|\mathbf{w})$). Same do other following models.</p>
<script type="math/tex; mode=display">
P(\mathbf{t}|\mathbf{w})=\Pi_{i}P(t_i|\mathbf{w})</script><h1 id="Hidden-Markove-models-HMMs"><a href="#Hidden-Markove-models-HMMs" class="headerlink" title="Hidden Markove models (HMMs)"></a>Hidden Markove models (HMMs)</h1><script type="math/tex; mode=display">
P(\mathbf{t}|\mathbf{w})=\Pi_{i}P(w_i|t_i)P(t_i|t_{i-1})</script><img src="/2018/10/20/ner/hmm.jpg">
<p>A brief deriving process is showed below:</p>
<script type="math/tex; mode=display">
P(\mathbf{t}|\mathbf{w})=P(\mathbf{w},\mathbf{t})/P(\mathbf{w})\propto P(\mathbf{w},\mathbf{t})=P(\mathbf{w}|\mathbf{t})P(\mathbf{t})=\Pi_{i}P(w_i|t_i)P(t_i|t_{i-1})</script><p>It is easy to know what we model here is the joint probability distribution $P(w,t)$. Therefore, <strong>HMM is a generative model</strong>. </p>
<p>The last step is because 2 assuptions. And the algorithm to find optimal $t$ to maximize $P(t|w)$ is <strong>Viterbi algorithm</strong>. Detailed infomation of the 2 assuptions and implement of the algorithm can be found <a href="http://wangyp.tech/2018/09/08/POS%20tag%20&amp;%20HMM/">here</a>.</p>
<h1 id="Maximunm-entropy-Markov-models-MEMMs"><a href="#Maximunm-entropy-Markov-models-MEMMs" class="headerlink" title="Maximunm entropy Markov models (MEMMs)"></a>Maximunm entropy Markov models (MEMMs)</h1><script type="math/tex; mode=display">
P(\mathbf{t}|\mathbf{w})=\Pi_{i}P(t_i|w_i,t_{i-1})</script><img src="/2018/10/20/ner/memm.jpg"> 
<p>What we model here is conditional probability distribution $P(t|w)$. Therefore, MEMM is a discriminative model.</p>
<p>Now, lets parameterize it:</p>
<script type="math/tex; mode=display">
P(t_i|w_i,t_{i-1})=softmax(\mathbf{S})_{i}</script><script type="math/tex; mode=display">
\mathbf{S}_i=s(t_i,t_{i-1},w_i)</script><script type="math/tex; mode=display">
s(t_i,t_{i-1},w_i)=\sum_{k}\lambda_kf_k(t_i,t_{i-1},w_i)</script><p>Where:</p>
<ul>
<li>$f_k$ is <strong>feature function</strong> (can be disigned freely, like emission-like feature, transition-like feature, etc.).</li>
<li>$\lambda$ is <strong>weight</strong> associated with each specific type of feature. </li>
<li>$s()$ is <strong>score function</strong>.</li>
<li>S is the vector consisting of all scores.</li>
<li>Nomalize S using softmax function then get the probability we want.</li>
</ul>
<p><strong>More about MEMMS</strong></p>
<p>Emission features can go across multiple observations. It is especially usefule for shallow parsing and NER tasks.</p>
<script type="math/tex; mode=display">
s(t_i,t_{i-1},w_i)=\sum_k\lambda_kf_k(t_i,t_{i-1},\mathbf{w})</script><img src="/2018/10/20/ner/memm2.jpg">
<p>[Does it has any relation with attention mechanism?]</p>
<h2 id="Conditional-Random-Field-CRF"><a href="#Conditional-Random-Field-CRF" class="headerlink" title="Conditional Random Field (CRF)"></a>Conditional Random Field (CRF)</h2><p>In MEMMs, we only consider the dependency among <script type="math/tex">t_{i}</script>,  <script type="math/tex">t_{i-1}</script>,  <script type="math/tex">\mathbf{w}</script>. But in some cases, like <strong>Label biased problem</strong>. Therefore, we want to model global dependency. This is the intuition of CRF.</p>
<script type="math/tex; mode=display">
P(\mathbf{t}|\mathbf{w})=\frac{S(\mathbf{t},\mathbf{w})}{\sum_{\mathbf{t}^\prime}\exp S(\mathbf{t}^{\prime},\mathbf{w})}</script><script type="math/tex; mode=display">
S(\mathbf{t},\mathbf{w})=\sum_i(\sum_k\lambda_kf_k(t_i,\mathbf{w}))+\sum_l\gamma_lg_l(t_i,t_{i-1},\mathbf{w}))</script><img src="/2018/10/20/ner/crf.jpg">
<p>Normalize globally, and score entire sequence directly.</p>
<p>What we model here is conditional probability distribution $P(t|w)$. Therefore, CRF is a discriminative model.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-tags" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://wangyp.tech/2018/09/12/Syntax Tree And CKY Parser/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Wang Yipeng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/myavatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yipeng's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/12/Syntax Tree And CKY Parser/" itemprop="url">Syntax Tree And CKY Parser [CS544 USC]</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-09-12T23:24:56-07:00">
                2018-09-12
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>The characteristics of the words and their relative location to each other allow us to generate a interpretation of a sentence even though we don’t really understand the sentence. It implies that the grammer structure of a sentence can help us understanding  it in some degree. Therefore, parsing a sentence into a syntax tree will be a very valuable work.</p>
<h1 id="CFG-and-Syntax-Tree"><a href="#CFG-and-Syntax-Tree" class="headerlink" title="CFG and Syntax Tree"></a>CFG and Syntax Tree</h1><p>Context-free grammer (CFG) is a certain type of grammer. A context-free grammar G is defined by the 4-tuple:</p>
<p><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2543ae859dd620f858299855868ae6c2ed066323" alt="G=(V,\Sigma ,R,S)"></p>
<p>V is a finite set of nonterminal characters, usually to be PoS tags.  Sigma is a finite set of terminals, the actural content of sentence. R is a finite set of rewrite rules.</p>
<script type="math/tex; mode=display">
V=\{S,NP,PP,VP,Vst,Det,V,N...\}</script><script type="math/tex; mode=display">
\Sigma=\{time,flies,like,an,arrow\}</script><script type="math/tex; mode=display">
R=\{S\rightarrow(NP,VP),S\rightarrow(Vst,NP),VP\rightarrow(V,NP)...\}</script><p>S is the start variable (or start symbol), used to represent the whole sentence (or program). It must be an element of V.</p>
<p>Based on G, we can build a syntax tree of a given sentence (sequence of words). A example syntax tree looks like:</p>
<img src="/2018/09/12/Syntax%20Tree%20And%20CKY%20Parser/syntax_tree.jpg">
<h1 id="CKY-Parser-CKY-Algorithm"><a href="#CKY-Parser-CKY-Algorithm" class="headerlink" title="CKY Parser (CKY Algorithm)"></a>CKY Parser (CKY Algorithm)</h1><h2 id="How-CKY-wroks"><a href="#How-CKY-wroks" class="headerlink" title="How CKY wroks"></a>How CKY wroks</h2><p>How to parse a sentence into a syntax tree (or syntax trees)? Yes, CKY parser (algorithm) is the tool we need. CKY algorithm is dynamic programming algorithm. </p>
<p>Let <em>s</em> denote the sentence (word sequence), <em>phrase(X, i, j)</em> denote the PoS tag for subsequence from xi to xj with <em>X</em> as root node:</p>
<script type="math/tex; mode=display">
s=(x_1,x_2,x_3,...,x_n),\ x_i\in\Sigma</script><script type="math/tex; mode=display">
phrase(X,i,j),\ X\in V,\ 1\leq i,j\leq n</script><p>The recursive formular is:</p>
<script type="math/tex; mode=display">
phrase(X,i,j)=rewrite(X,Y,Z),\ phrase(Y,i,k),\ phrase(Z,k+1,j)</script><script type="math/tex; mode=display">
X\rightarrow(Y,Z)\in R</script><script type="math/tex; mode=display">
k\in\{i,i+1,...,j-1\}</script><p>The initial state is:</p>
<script type="math/tex; mode=display">
phrase(X,i,i)=rewrite(X,W),\ word(W,i,i)</script><script type="math/tex; mode=display">
W\in\Sigma</script><p>If we traceback from the top, <em>phrase(X, 1, n)</em>, we can get all  possible syntax of the given sentence under the given CFG. Please refer to the CSCI544-slides for the pseudocode. My notation may be a little different from the slides for I am trying to make it more understandable &gt;_&lt;’’</p>
<h1 id="Probabilistic-CKY-Parser-Vanilla-PCFGs"><a href="#Probabilistic-CKY-Parser-Vanilla-PCFGs" class="headerlink" title="Probabilistic CKY Parser (Vanilla PCFGs)"></a>Probabilistic CKY Parser (Vanilla PCFGs)</h1><h2 id="How-PCKY-works"><a href="#How-PCKY-works" class="headerlink" title="How PCKY works"></a>How PCKY works</h2><p>For a given sentence and a given CFG, there meybe (actually usually) more than one possible syntax tree. It will cause ambiguity. If we assign a ‘weight’ to every rule, we can calculate the weight of every possible tree and pick one from them withour ambiguity.</p>
<p>Then it comes a question, how to define a ‘weight’ and how to pick it. If we define the weight of a rule a its possibility to occur, then all possible syntax tree will have a distribution. Based on this definition, we have a modified algorithm, probabilistic CKY parser (or PCFG).</p>
<p>For given CFG, define T as all possible syntax tree, we are going to get p(t), the distribution over T and choose the syntax tree with the largest probability. </p>
<p>Let <em>s</em> denote the sentence (word sequence), <em>T(X, i, j)</em> denote all possible syntax trees for subsequence from xi to xj with <em>X</em> as root node. Then define</p>
<script type="math/tex; mode=display">
\pi(X,i,j)=\max_{t\in T(X,i,j)}p(t)</script><p>Initial state:</p>
<script type="math/tex; mode=display">
\pi(X,i,i)=
\left\{
    \begin{aligned}
    &p(X\rightarrow x_i)&if\ X\rightarrow x_i\in R\\
    &0&otherwise 
    \end{aligned}
\right.</script><script type="math/tex; mode=display">
p(X\rightarrow x_i)=\frac{count(X\rightarrow x_i)}{count(X)}</script><p>The recursive formular is:</p>
<script type="math/tex; mode=display">
\pi(X,i,j)=\max_{X\rightarrow(Y,Z)\in R,\ i\le k<j}p(X\rightarrow(Y,Z))\times\pi(Y,i,k)\times\pi(Z,k+1,j)</script><script type="math/tex; mode=display">
p(X\rightarrow(Y,Z))=\frac{count(X\rightarrow(Y,Z))}{count(X)}</script><p>At last, the syntax tree we want is:</p>
<script type="math/tex; mode=display">
T_{best}=\mathop{\arg\max}_{t\in T(X,1,n),\ x\in V}p(t)</script><h2 id="Applicable-tasks"><a href="#Applicable-tasks" class="headerlink" title="Applicable tasks"></a>Applicable tasks</h2><ul>
<li>Given a sentence and a CFG, to find the best parsing (syntax tree) of the sentence.</li>
<li>Given a CFG, to get the probability of a sentence to be generated.</li>
</ul>
<p>In task2, we need to add up probabilities of all possible parsing result of the sentence.</p>
<h2 id="Generalizing"><a href="#Generalizing" class="headerlink" title="Generalizing"></a>Generalizing</h2><p>If we apply a -log() on both sides of the recursive formular, we get <strong>weighted CKY parser</strong>. And what we want becomes the syntax tree with the lowest weight (-log cost). PCKY is actually a special case of WCKY.</p>
<p>WCKY can be generalized into <strong>semiring-weighted CKY</strong> (the most general algorithm). Just use semiring operation (should obey <strong>semiring axioms</strong>) to replace original specific operation.  Deatails could be found in CSCI544-slides. </p>
<h2 id="Drawback-and-improvement"><a href="#Drawback-and-improvement" class="headerlink" title="Drawback and improvement"></a>Drawback and improvement</h2><p>The information of word relations does not contribute much to the building of the whole syntax tree of a sentence. <strong><em>(I really hope u guys could give some better explanation for this)</em></strong></p>
<p>Instead of using PoS tags only (elements in V), we use tuple <strong>(head word, PoS tag)</strong> for each nonterminal character.</p>
<p>original:</p>
<script type="math/tex; mode=display">
S\rightarrow NP,\ VP</script><p>After <strong>lexicalized</strong>:</p>
<script type="math/tex; mode=display">
S(loves,VB)\rightarrow NP(John,NNP),\ VP(loves,VB)</script><p><strong>References:</strong></p>
<p><a href="https://github.com/isi-nlp/csci_544_fa_18_slides/blob/master/pcfg_0914.pptx" target="_blank" rel="noopener">https://github.com/isi-nlp/csci_544_fa_18_slides/blob/master/pcfg_0914.pptx</a></p>
<p><a href="https://blog.csdn.net/qq_37171771/article/details/79342819" target="_blank" rel="noopener">https://blog.csdn.net/qq_37171771/article/details/79342819)</a></p>
<p><a href="https://en.wikipedia.org/wiki/Probabilistic_context-free_grammar" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Probabilistic_context-free_grammar</a></p>
<p><a href="http://ccl.pku.edu.cn/doubtfire/Course/Computational%20Linguistics/contents/CYK_parsing.pdf" target="_blank" rel="noopener">http://ccl.pku.edu.cn/doubtfire/Course/Computational%20Linguistics/contents/CYK_parsing.pdf</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-tags" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://wangyp.tech/2018/09/08/POS tag & HMM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Wang Yipeng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/myavatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yipeng's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/08/POS tag & HMM/" itemprop="url">POS tag & HMM [CS544 USC]</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-09-08T09:33:13-07:00">
                2018-09-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="POS-Tag"><a href="#POS-Tag" class="headerlink" title="POS Tag"></a>POS Tag</h1><p>POS is short for “part of speech” which also means <strong>syntactic categories</strong>. POS tagging is the first step toward full syntactic analysis and it is often good features for other tasks.</p>
<p>The tag of a word depends on the word itself and the tags of surrounding words. Such a situation can be descripbed by HMM probabilistically.</p>
<ul>
<li>Open class words: It is also called content words. It could be nouns, verbs, adjectives or adverbs usually. There is no limit to what they are and can be added all the times.</li>
<li>Closed class words: It is also called function words. It could be pronouns, determiners, prepositions or connectives usually. There are a limit number of these words. </li>
</ul>
<h1 id="Different-Sets-of-POS-Tags"><a href="#Different-Sets-of-POS-Tags" class="headerlink" title="Different Sets of POS Tags"></a>Different Sets of POS Tags</h1><p>There are many different sets of POS tags based on different linguistic and practical considerations. We can choose to use one of them according to our specific task. </p>
<p>Commonly used tag sets for English usually have 40 to 100 tag types. The <strong><a href="https://www.clips.uantwerpen.be/pages/mbsp-tags" target="_blank" rel="noopener">Penn Treebank</a></strong> has 45 tags.</p>
<h1 id="Modeling-POS-Tagging-with-HMM-1"><a href="#Modeling-POS-Tagging-with-HMM-1" class="headerlink" title="Modeling POS Tagging with HMM[1]"></a>Modeling POS Tagging with HMM[1]</h1><p>What we are interested in is P(W|T) and P(T) where W and T are sequences of length N.</p>
<ul>
<li>Assumption 1: Each tag is conditioned only on the previous tag. This is so-called <strong>Markov Property</strong></li>
<li>Assumption 2: Each word is conditioned only on its tag.</li>
</ul>
<p>Based on these two assumptions, we can use HMM to describe the POS tagging process. </p>
<script type="math/tex; mode=display">
argmax_{T}P(T|W)=argmax_{T}P(W|T)P(T)</script><p>To solve this optimization problem, we can use <strong>Greedy Algorithm</strong>, i.e. at each time step, compute the possibility under each possible tag given the previous tag and choose the best one. It is easy to know the time complexity of greedy algorithm is <strong>O(|T|*N)</strong>.</p>
<p>Greedy algorithm’s result is suboptimal. To get the optimal tag sequence we need <strong>Viterbi Algorithm[2]</strong>. If we use T to denote tag sequence, W to denote word sequence, and v[T, W] to denote be the probability of the best T-W pair, the algorithm can be described with formulas below: </p>
<script type="math/tex; mode=display">
T_{1:i}=[t_1,t_2,...,t_i]</script><script type="math/tex; mode=display">
W_{1:i}=[w_1,w_2,...,w_i]</script><script type="math/tex; mode=display">
v[T_{1:i},W_{1:i}]=max_{t_{i}}v[T_{1:i-1},W_{1:i-1}]\times P(t_{i}|t_{i-1})\times P(W_{i}|t_{i})</script><p><strong>[1] Hidden Markov Model (HMM)</strong> is a statistical <a href="https://en.wikipedia.org/wiki/Markov_model" target="_blank" rel="noopener">Markov model</a> in which the system being modeled is assumed to be a <a href="https://en.wikipedia.org/wiki/Markov_process" target="_blank" rel="noopener">Markov process</a> with unobserved (i.e. <em>hidden</em>) states.</p>
<p>In simpler Markov models (like a <a href="https://en.wikipedia.org/wiki/Markov_chain" target="_blank" rel="noopener">Markov chain</a>), the state is directly visible to the observer, and therefore the state transition probabilities are the only parameters, while in the hidden Markov model, <strong><em>the state (i.e. POS tag in our discussion)</em></strong> is not directly visible, but the <strong><em>output (i.e. Word)</em></strong>, dependent on the state, is visible. Each state has a probability distribution over the possible output tokens. Therefore, the sequence of tokens generated by an HMM gives some information about the sequence of states.</p>
<p><strong>[2] Viterbi algorithm</strong> is a <a href="https://en.wikipedia.org/wiki/Dynamic_programming" target="_blank" rel="noopener">dynamic programming</a> algorithm for finding the most likely sequence of hidden states—called the <strong>Viterbi path</strong>—that results in a sequence of observed events, especially in the context of Markov information sources and hidden Markov models.</p>
<p>Intuition: the best path of length i ending in state must include the best path of length i-1 to the previous state.</p>
<p>Baisc steps: Best path of length i-1 to each state —&gt; Extend each path to state t and find the best one —&gt; Best path of length i in state t. </p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-tags" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://wangyp.tech/2018/07/29/What gonna happen next/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Wang Yipeng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/myavatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yipeng's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/29/What gonna happen next/" itemprop="url">What gonna happen next?</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-07-29T02:16:45-07:00">
                2018-07-29
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Event-Prediction-Based-on-“Event-Embedding”"><a href="#Event-Prediction-Based-on-“Event-Embedding”" class="headerlink" title="Event Prediction Based on “Event Embedding”"></a>Event Prediction Based on “Event Embedding”</h1><p><a href="https://aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11995/12015" target="_blank" rel="noopener">Original paper by Mark Granroth-Wilding and Stephen Clark: What Happens Next? Event Prediction Using a Compositional Neural Network Model</a></p>
<h1 id="Recall"><a href="#Recall" class="headerlink" title="Recall"></a>Recall</h1><p>Let’s recall the original work of Chambers and Jurafsky in 2008. In that <a href="https://nlp.stanford.edu/pubs/narrative-chains08.pdf" target="_blank" rel="noopener">paper</a>, an <strong><em>event</em></strong> is defined as a pair of the predicate <strong><em>(verb)</em></strong> and  the grammatical dependency <strong><em>(subj, obj, or iobj)</em></strong>. So an event is expressed in a form of <strong><em>predicate-GR</em></strong>.</p>
<p>C&amp;J looks only at the co-occurrences of predicate-GRs. The prediction is totally based on statistical analysis. But there are still some obvious problems:</p>
<ul>
<li>A lot of information is lost when we transfer an entity of an event into GR. The meaning of an event could be drastically changed by its entities. <strong><em>Eg1. (perform, play<subj>) VS. (perform surgery<subj>); Eg2. (go, on holiday<subj>) VS. (go, to church<subj>)</subj></subj></subj></subj></em></strong>  </li>
<li>The model are not able to assign scores to unobserved pairs. And only events with frequent predicates could be scored accurately.</li>
</ul>
<h1 id="Insight"><a href="#Insight" class="headerlink" title="Insight"></a>Insight</h1><p>Inspired by the paper word2vec (2013), Two authors (Mark &amp; Stephen) use contininuous vector embeddings of words and events to replace the original definition on an event (predicate-GR). Then M&amp;S train a neural model to score any input event-vector pairs.</p>
<h1 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h1><p><strong>Verb-only model</strong></p>
<p>M&amp;S represent an event using the vector <em>(MIKOLOV-VERB)</em> for its verb. Vectors of events are summed up as the description of the context (i.e the event chain we reffered in my last post). Finally, compute the cosine similarity between the context and candidate event.</p>
<script type="math/tex; mode=display">
s(c)=cosine(\sum^{n-1}_{j=0}W_{p(e_j)},W_{p(c)})</script><p><strong>Verb-argument model</strong></p>
<p>Just a little difference from the model above. An event is repersened as the sum of the vectors for its verb and each of its arguments. Other definitions or methods keep unchanged.</p>
<script type="math/tex; mode=display">
s(c)=cosine(\sum^{n-1}_{j=0}[W_{p(e_j)}+W_{a_0(e_j)}+W_{a_1(e_j)}+W_{a_2(e_j)}],W_{p(c)}+W_{a_0(c)}+W_{a_1(c)}+W_{a_2(c)})</script><p>Actually, neither of two models above are adopted a last, because the roles of verb and arguments are obvious different. It is not proper to combine them linearly as the description of an event or a context. M&amp;S apply neural composition model to implement nonlinear combination of the two kinds of vectors. Such a model is called <strong><em>EVENT-COMP</em></strong>, we will talk about it later.</p>
<p><strong>Embedding model</strong></p>
<p>In the two models above, the word-vectors we used is from word-vector set provided in the work of <a href="https://arxiv.org/pdf/1301.3781.pdf" target="_blank" rel="noopener">Mikolov et al. (2013a)</a>. These word-vectors are trained based on n-gram, center word and context. </p>
<p>Inspired by this, and based on the same idea, M&amp;S treat each event’s predicate-GR as a word and each trianing chain as a sentence, and then train a skip-gram model (with hierarchical sampling, using a window size of 5 and vector size of 300), i.e. <strong><em>WORD2VEC-PRED</em></strong>.</p>
<p>Besides the approach above, M&amp;S also treat predicate and arguments as separate words and place them in the ‘sentence’. Using the similar method, they trained another model <strong><em>WORD2VEC-PRED+ARG</em></strong>, this is also the embedding model adopted finally.</p>
<p><strong>Neural Compositional Representations</strong></p>
<p>We want to train a neural network <strong><em>EVENT-COMP</em></strong> to learn a non-linear composition of predicates and arguments into an event representation and a similarity model to determine the similarity between two vectorized events.</p>
<ul>
<li><strong>Input Layer</strong> The vectors of the predicate and arguments coresponding to an event. Empty  arguments are filled with zero vectors.</li>
<li><strong>Argument Composition Layer</strong> It consists of a stack of <a href="http://deeplearning.net/tutorial/dA.html" target="_blank" rel="noopener"><strong><em>denoising autoencoders</em></strong></a>. It produces a low-dimensional representation of the event.</li>
<li><strong>Event Composition Layer</strong> It consists of fully-connected layers. It produces a single output value <strong><em>coherence score (coh)</em></strong>. The coh represents how confident two input events are from a same chain.</li>
</ul>
<img src="/2018/07/29/What%20gonna%20happen%20next/event_comp.jpg" title="event_comp">
<p>Some training iterations are performed to update only the parameters of event composition layer; Further iterations then update all parameters. (Parameters for 2 events in input layer and argument composition layer are fixed to be same)</p>
<ul>
<li><strong>Loss function</strong><script type="math/tex; mode=display">
\frac{1}{m}\sum^{m}_{i=1}-log[p_i\times coh(e^0_i, e^1_i)+(1-p_i)\times(1-coh(e^0_i, e^1_i))]+\lambda L(\theta)</script>Pi = 1 for positive samples (2 events are from a same chain), and Pi = 0 for negative samples (2 events are from different chain). L is an l2 regularization term. The object is to minimize the loss function.</li>
</ul>
<h1 id="What-gonna-happen-next"><a href="#What-gonna-happen-next" class="headerlink" title="What gonna happen next?"></a>What gonna happen next?</h1><p>After training all the models. Given a chain of events, we can compute a <strong><em>coh</em></strong> for a candidate event and each event in the chain. Then sum up these scores. So that we can choose a candidate event with the largest total score as our prediction.</p>
<script type="math/tex; mode=display">
s(c)=\frac{1}{n}\sum^{n-1}_{j=0}coh(c, e_j)</script><h1 id="Extra"><a href="#Extra" class="headerlink" title="Extra"></a>Extra</h1><p>M&amp;S also provide a effective evaluation method to test the performance of event prediction models called <strong><em>Multiple Choice Version of the Narrative Cloze Task (MCNC)</em></strong>. I am not gonna talk about it here, because it works like what it looks like.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          
            <img class="site-author-image" itemprop="image"
              src="/images/myavatar.jpg"
              alt="Wang Yipeng" />
          
            <p class="site-author-name" itemprop="name">Wang Yipeng</p>
            <p class="site-description motion-element" itemprop="description">Sharing of my daily work.</p>
        </div>

        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
            
              <a href="/archives/">
            
                <span class="site-state-item-count">10</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">9</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="mailto:yipengwa@usc.edu" target="_blank" title="E-Mail">
                  
                    <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Wang Yipeng</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div>

  <span class="post-meta-divider">|</span>

  <div class="theme-info">Theme &mdash; <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.2</div>


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.2"></script>



  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>


  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  




	





  





  








  





  

  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
