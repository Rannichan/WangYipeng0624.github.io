<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="Management of skills and knowledge.">
<meta property="og:type" content="website">
<meta property="og:title" content="Yipeng&#39;s Blog">
<meta property="og:url" content="http://wangyp.tech/index.html">
<meta property="og:site_name" content="Yipeng&#39;s Blog">
<meta property="og:description" content="Management of skills and knowledge.">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Yipeng&#39;s Blog">
<meta name="twitter:description" content="Management of skills and knowledge.">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.2',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://wangyp.tech/"/>





  <title>Yipeng's Blog</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Yipeng's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">God prefers concision.</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-tags" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://wangyp.tech/2018/07/06/LeakGAN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Wang Yipeng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yipeng's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/06/LeakGAN/" itemprop="url">LeakGAN - a Long Text Generating Model</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-07-06T20:07:34+08:00">
                2018-07-06
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>LeakGAN is a strengthen version of SeqGAN. Instead of using reward form Discriminator to train generator, LeakGAN add high-level feature (leaked information) extracted by CNN in Discriminator into the training of Generator, which greatly improve the performance of generating long text. Some other tricks are also included in the experioment of LeakGAN aiming to solving some problems showed in SeqGAN. </p>
<h1 id="Core-problem-—-Long-text-generating"><a href="#Core-problem-—-Long-text-generating" class="headerlink" title="Core problem — Long text generating"></a>Core problem — Long text generating</h1><p>In my previous experiment of SeqGAN, some generating results are shown bellow:</p>
<ul>
<li>: ( data is stored in internal server and can not be fetched out</li>
</ul>
<p>From the examples above, we can find the result is very bad. It mainly because:</p>
<ol>
<li>Information contained in Reward is not enough for generating long test.</li>
<li>LSTM used in Generator is not capable to form long-term memory.</li>
</ol>
<p>To solve the 1st problem, LeakGAN uses feature map outputted by Discriminator’s CNN-Maxpooling layer as leaked information to train Manager to generate goal vectors. Then, Worker uses goal vectors and Discriminator’s reward together to generate sequences.</p>
<h2 id="Model-structure"><a href="#Model-structure" class="headerlink" title="Model structure"></a>Model structure</h2><img src="/2018/07/06/LeakGAN/model_structure.jpg">
<p><strong>Discriminator-Feature Extractor CNN</strong></p>
<script type="math/tex; mode=display">f_t=\digamma(S_t; \phi_f)</script><p><strong>Generator-Manager LSTM</strong></p>
<script type="math/tex; mode=display">\hat{g_t}, h^m_t=M(f_t, h^m_{t-1}; \theta_m)</script><script type="math/tex; mode=display">g_t=\hat{g_t}/||\hat{g_t}||</script><script type="math/tex; mode=display">w_t=W_{\psi}(\sum^c_{i=1}g_{t-i})</script><p><strong>Generator-Worker LSTM</strong></p>
<script type="math/tex; mode=display">O_t, h_t^{w}=W(x_t, h^w_{t-1}; \theta_w)</script><script type="math/tex; mode=display">G_{\theta}(x_{t+1}|s_t)=softmax(O_t\centerdot w_t/\alpha)</script><h2 id="Model-training"><a href="#Model-training" class="headerlink" title="Model training"></a>Model training</h2><h3 id="Generator-training"><a href="#Generator-training" class="headerlink" title="Generator training"></a>Generator training</h3><p>Manager and Worker are trained alternatively. One should be fixed when the other is in training. Manager is trained to generate advantageous directions. Worker is trained to follow the directions.</p>
<p><strong>Gradient of Manager <script type="math/tex">\nabla_{\theta_m}g_t</script></strong></p>
<ul>
<li><p><em>Expected reward under current policy</em></p>
<script type="math/tex; mode=display">E[r_t]=Q(f_t, g_t)=Q_F(s_t, g_t)</script></li>
<li><p><em>Cosine similarity between change of feature representation and goal vector</em></p>
<script type="math/tex; mode=display">d_{cos}(f_{t+c}-f_t, g_t(\theta_m))</script></li>
<li><p><em>Policy gradient</em></p>
<script type="math/tex; mode=display">\nabla_{\theta_m}g_t=-Q_F(s_t, g_t)\nabla_{\theta_m}d_{cos}(f_{t+c}-f_t, g_t(\theta_m))</script></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">self.goal_loss = -tf.reduce_sum(</span><br><span class="line">  	tf.multiply(self.reward,</span><br><span class="line">                <span class="number">1</span>-tf.losses.cosine_distance(</span><br><span class="line">                  	tf.nn.l2_normalize(self.sub_feature,<span class="number">2</span>),</span><br><span class="line">                  	tf.nn.l2_normalize(self.real_goal_array,<span class="number">2</span>), </span><br><span class="line">                  	<span class="number">2</span></span><br><span class="line">                )</span><br><span class="line">    )</span><br><span class="line">) / (self.sequence_length * self.batch_size / self.step_size)</span><br></pre></td></tr></table></figure>
<p><strong>Gradient of Worker</strong></p>
<ul>
<li><p><em>Intrinsic reward</em></p>
<script type="math/tex; mode=display">r^I_t=\frac{1}{c}\sum^c_{i=1}d_{cos}(f_t-f_{t-i}, g_{t-i})</script></li>
</ul>
<ul>
<li><p><em>Expected intrinsic reward over all possible actions</em></p>
<script type="math/tex; mode=display">\sum_{x_t}r^I_tW(x_t|s_{t-1}; \theta_w)</script></li>
<li><p><em>Expected intrinsic reward over all possible actions &amp; all current states</em></p>
<script type="math/tex; mode=display">E_{s_{t-1}\sim G}[\sum_{x_t}r^I_tW(x_t|s_{t-1}; \theta_w)]</script></li>
<li><p><em>Policy gradient</em></p>
<script type="math/tex; mode=display">\nabla_{\theta_w}E_{s_{t-1}\sim G}[\sum_{x_t}r^I_tW(x_t|s_{t-1}; \theta_w)]</script><script type="math/tex; mode=display">=E_{s_{t-1}\sim G}[\sum_{x_t}r^I_t\nabla_{\theta_w}W]</script><script type="math/tex; mode=display">=E_{s_{t-1}\sim G}[\sum_{x_t}r^I_tW\frac{\nabla_{\theta_w}W}{W}]</script><script type="math/tex; mode=display">=E_{s_{t-1}\sim G, x_t\sim W}[r^I_t\nabla_{\theta_w}logW]</script></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">self.Worker_Reward = <span class="number">1</span>-tf.losses.cosine_distance(tf.nn.l2_normalize(self.all_sub_features,<span class="number">2</span>),</span><br><span class="line">                                                 tf.nn.l2_normalize(self.all_sub_goals,<span class="number">2</span>), </span><br><span class="line">                                                 <span class="number">2</span>)</span><br><span class="line">self.worker_loss = -tf.reduce_sum(</span><br><span class="line">  	tf.multiply(</span><br><span class="line">      	self.Worker_Reward,</span><br><span class="line">      	tf.one_hot(tf.to_int32(tf.reshape(self.x, [<span class="number">-1</span>])), self.vocab_size, <span class="number">1.0</span>, <span class="number">0.0</span>) * \</span><br><span class="line">      	tf.log(tf.clip_by_value(tf.reshape(self.g_predictions, [<span class="number">-1</span>, self.vocab_size]), <span class="number">1e-20</span>, \			<span class="number">1.0</span>))</span><br><span class="line">    )</span><br><span class="line">) / (self.sequence_length * self.batch_size)</span><br></pre></td></tr></table></figure>
<p><strong>Pretraining of Manager</strong></p>
<p>States of real text:</p>
<script type="math/tex; mode=display">\hat{s_t}</script><script type="math/tex; mode=display">\hat{f_t}=F(\hat{s_t})</script><script type="math/tex; mode=display">\nabla^{pre}_{\theta_m}g_t = -\nabla_{\theta_m}d_{cos}(\hat{f_{t+c}}-\hat{f_t}, g_t(\theta_m))</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">sub_feature = tf.cond(i % step_size &gt; <span class="number">0</span>, </span><br><span class="line">                      <span class="keyword">lambda</span>: sub_feature,</span><br><span class="line">                      <span class="keyword">lambda</span>: tf.cond(i &gt; <span class="number">0</span>, </span><br><span class="line">                                      <span class="keyword">lambda</span>:sub_feature.write(i/step_size<span class="number">-1</span>,tf.subtract(feature,feature_array.read(i - step_size))),</span><br><span class="line">                    				  <span class="keyword">lambda</span>: sub_feature</span><br><span class="line">                                     )</span><br><span class="line">                     )</span><br><span class="line"><span class="string">"""Pretrain loop..."""</span></span><br><span class="line">self.sub_feature = self.sub_feature.stack() <span class="comment"># seq_length x batch_size x num_filter</span></span><br><span class="line">self.sub_feature = tf.transpose(self.sub_feature, perm=[<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>])</span><br><span class="line">self.real_goal_array = self.real_goal_array.stack()</span><br><span class="line">self.real_goal_array = tf.transpose(self.real_goal_array, perm=[<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>])</span><br><span class="line">self.pretrain_goal_loss = -tf.reduce_sum(</span><br><span class="line">	<span class="number">1</span> - tf.losses.cosine_distance(</span><br><span class="line">    	tf.nn.l2_normalize(self.sub_feature,<span class="number">2</span>),</span><br><span class="line">      	tf.nn.l2_normalize(self.real_goal_array,<span class="number">2</span>),</span><br><span class="line">		<span class="number">2</span></span><br><span class="line">    )</span><br><span class="line">) / (self.sequence_length * self.batch_size/self.step_size)</span><br></pre></td></tr></table></figure>
<p><strong>Pretraining of Worker</strong></p>
<p><em>MLE</em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">g_predictions = tf.cond(</span><br><span class="line">                i&gt;<span class="number">0</span>,</span><br><span class="line">                <span class="keyword">lambda</span> :g_predictions.write(i<span class="number">-1</span>, tf.nn.softmax(x_logits)),</span><br><span class="line">                <span class="keyword">lambda</span> :g_predictions)</span><br><span class="line"><span class="string">"""Pretrain loop..."""</span></span><br><span class="line">self.g_predictions = tf.transpose(self.g_predictions.stack(), perm=[<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>])</span><br><span class="line">self.cross_entropy = tf.reduce_sum(</span><br><span class="line">  		self.g_predictions * tf.log(tf.clip_by_value(self.g_predictions, <span class="number">1e-20</span>, <span class="number">1.0</span>))</span><br><span class="line">	) / (self.batch_size * self.sequence_length * self.vocab_size)</span><br><span class="line">self.pretrain_worker_loss = -tf.reduce_sum(</span><br><span class="line">  		tf.one_hot(tf.to_int32(tf.reshape(self.x, [<span class="number">-1</span>])), self.vocab_size, <span class="number">1.0</span>, <span class="number">0.0</span>)* \</span><br><span class="line">  		tf.log(tf.clip_by_value(</span><br><span class="line">        	tf.reshape(self.g_predictions, [<span class="number">-1</span>, self.vocab_size]), </span><br><span class="line">          	<span class="number">1e-20</span>, <span class="number">1.0</span>)</span><br><span class="line">        )</span><br><span class="line">	) / (self.sequence_length * self.batch_size)</span><br></pre></td></tr></table></figure>
<h3 id="Discriminator-Training"><a href="#Discriminator-Training" class="headerlink" title="Discriminator Training"></a>Discriminator Training</h3><p><strong>Pretraining</strong></p>
<p>Adam gradient descent with cross entropy loss </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'D_update'</span>):</span><br><span class="line">	self.D_l2_loss = tf.constant(<span class="number">0.0</span>)</span><br><span class="line">	self.FeatureExtractor_unit = self.FeatureExtractor()</span><br><span class="line">    <span class="comment"># Train for Discriminator</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"feature"</span>) <span class="keyword">as</span> self.feature_scope:</span><br><span class="line">    	D_feature = self.FeatureExtractor_unit(self.D_input_x,</span><br><span class="line">                                               self.dropout_keep_prob)</span><br><span class="line">        self.feature_scope.reuse_variables()</span><br><span class="line">    D_scores, D_predictions,self.ypred_for_auc = self.classification(D_feature)</span><br><span class="line">    losses = tf.nn.softmax_cross_entropy_with_logits(logits=D_scores, </span><br><span class="line">                                                     labels=self.D_input_y)</span><br><span class="line">    self.D_loss = tf.reduce_mean(losses) + self.l2_reg_lambda * self.D_l2_loss</span><br><span class="line">    self.D_params = [param <span class="keyword">for</span> param <span class="keyword">in</span> tf.trainable_variables() \ </span><br><span class="line">                     <span class="keyword">if</span> <span class="string">'Discriminator'</span> <span class="keyword">or</span> <span class="string">'FeatureExtractor'</span> <span class="keyword">in</span> param.name]</span><br><span class="line">    d_optimizer = tf.train.AdamOptimizer(<span class="number">5e-5</span>)</span><br><span class="line">    D_grads_and_vars = d_optimizer.compute_gradients(self.D_loss, </span><br><span class="line">                                                     self.D_params, </span><br><span class="line">                                                     aggregation_method=<span class="number">2</span>)</span><br><span class="line">    self.D_train_op = d_optimizer.apply_gradients(D_grads_and_vars)</span><br></pre></td></tr></table></figure>
<hr>
<h1 id="Problem-2-—-Gradient-Vanishing"><a href="#Problem-2-—-Gradient-Vanishing" class="headerlink" title="Problem 2 — Gradient Vanishing"></a>Problem 2 — Gradient Vanishing</h1><p>There will be a severe gradient vanishing problem during the training of GAN when D is much stronger than  G. The reward is too small to update the parameters in G. Therefore it needs to  be rescaled.</p>
<p><strong>Trick — Bootstrapped Rescaled Activation</strong></p>
<p>Reward matrix is <script type="math/tex">R_{B*T}</script> with batch-size B and sequence-length T.</p>
<p>Rescale it by doing <script type="math/tex">R_{i,t} = \sigma(\delta\centerdot(0.5-\frac{rank(i)}{B}))</script>.</p>
<ul>
<li>Expectation and variance of the reward in each mini-batch are constant!</li>
<li>In general, all ranking model can prevent the gradient vanishing problem (<a href="http://papers.nips.cc/paper/6908-adversarial-ranking-for-language-generation.pdf" target="_blank" rel="noopener">More explanation — RankGAN</a>).</li>
</ul>
<h1 id="Problem-3-—-Mode-Collapse"><a href="#Problem-3-—-Mode-Collapse" class="headerlink" title="Problem 3 — Mode Collapse"></a>Problem 3 — Mode Collapse</h1><p>These are several examples generated by SeqGAN I trained before:</p>
<ul>
<li>: ( data is stored in internal server and can not be fetched out</li>
</ul>
<p>As we can see, the generator is inclined to produce similar or  same sequences, which is called <strong>Mode Collapse</strong>. It is because… You can find more about mode collapse from <a href="https://www.quora.com/What-causes-mode-collapse-in-GANs" target="_blank" rel="noopener">Quora</a></p>
<p><strong>Trick — Interleaved Training</strong></p>
<p>Instead of using full GAN training (unsupervised), it adopt a MLE training (supervised) after 15 epochs of GAN training to prevent the Worker get into some bad local minimums.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> total_batch % <span class="number">15</span> == <span class="number">0</span>:</span><br><span class="line">	<span class="keyword">for</span> epoch <span class="keyword">in</span> xrange(<span class="number">1</span>):</span><br><span class="line">    loss = pre_train_epoch(sess, leakgan, gen_data_loader)</span><br></pre></td></tr></table></figure>
<h1 id="Problem-4-—-Balance-the-exploration-and-exploitation"><a href="#Problem-4-—-Balance-the-exploration-and-exploitation" class="headerlink" title="Problem 4* — Balance the exploration and exploitation"></a>Problem 4* — Balance the exploration and exploitation</h1><p><strong>Trick — Temperature Control</strong></p>
<p>Higher temperature for training the model — exploration</p>
<p>Lower temperature for generating samples — exploitation</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-tags" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://wangyp.tech/2018/06/29/SeqGAN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Wang Yipeng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yipeng's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/29/SeqGAN/" itemprop="url">用GAN生成对抗网络和RL强化学习手段生成自然语言序列</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-29T12:32:43+08:00">
                2018-06-29
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><h2 id="GAN的思想"><a href="#GAN的思想" class="headerlink" title="GAN的思想"></a>GAN的思想</h2><ul>
<li>组件：生成器G（Generator），鉴别器D（Discriminator）</li>
<li>对抗：D用来鉴别真实数据和生成数据，G不断进化自身来生成可以迷惑D的数据</li>
<li>目标：G利用D的反向传播不断更新自身参数，最后可以生成以假乱真的数据</li>
</ul>
<h2 id="GAN的不足之处"><a href="#GAN的不足之处" class="headerlink" title="GAN的不足之处"></a>GAN的不足之处</h2><ul>
<li>G通常使用的是LSTM，那么G传递给D的是一堆离散值序列（即每一个LSTM单元的输出经过softmax之后再取argmax或者基于概率采样得到一个具体的单词），这导致D的损失函数不是连续可微的，这使得梯度下架很难处理。</li>
<li>D只能评估出整个生成序列的score/loss，不能够细化到去评估当前生成token的好坏和对后面生成的影响</li>
</ul>
<h2 id="RL的功能"><a href="#RL的功能" class="headerlink" title="RL的功能"></a>RL的功能</h2><ul>
<li>Policy Gradient取代反向传播：利用reward作为反馈，增加具有高reward的动作（G生成一个单词）出现的概率，减小reward小的动作出现的概率，以此进行梯度训练，更新参数，不再需要依赖于D的反向传播来更新参数，</li>
<li>Roll-out Model生成完整序列：将当前生成的不完整的序列补全为完整的序列，从而可以直接输入D得到一个score/loss（reward）</li>
</ul>
<h1 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h1><h2 id="整体框架"><a href="#整体框架" class="headerlink" title="整体框架"></a>整体框架</h2><img src="/2018/06/29/SeqGAN/model_structure.jpg">
<h2 id="Generator-RNN-with-LSTM"><a href="#Generator-RNN-with-LSTM" class="headerlink" title="Generator - RNN with LSTM"></a>Generator - RNN with LSTM</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">generator = Generator(vocab_size, BATCH_SIZE, EMB_DIM, HIDDEN_DIM, SEQ_LENGTH, START_TOKEN)</span><br></pre></td></tr></table></figure>
<img src="/2018/06/29/SeqGAN/generator.jpg">
<h2 id="Discriminator-CNN-with-Highway"><a href="#Discriminator-CNN-with-Highway" class="headerlink" title="Discriminator - CNN with Highway"></a>Discriminator - CNN with Highway</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">discriminator = Discriminator(sequence_length=<span class="number">20</span>, num_classes=<span class="number">2</span>, vocab_size=vocab_size, embedding_size=dis_embedding_dim, filter_sizes=dis_filter_sizes, num_filters=dis_num_filters, l2_reg_lambda=dis_l2_reg_lambda)</span><br></pre></td></tr></table></figure>
<img src="/2018/06/29/SeqGAN/discriminator.jpg">
<h2 id="Roll-out-Model"><a href="#Roll-out-Model" class="headerlink" title="Roll-out Model"></a>Roll-out Model</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rollout = ROLLOUT(generator, <span class="number">0.8</span>)</span><br></pre></td></tr></table></figure>
<p>共享Generator的网络结构与参数</p>
<h2 id="Oracle-Model-Target-LSTM"><a href="#Oracle-Model-Target-LSTM" class="headerlink" title="Oracle Model - Target LSTM"></a>Oracle Model - Target LSTM</h2><p>该模型被用作参考模型（ human observer for real-world problems），其作用是：</p>
<ol>
<li>被用来生成训练样本（正样本，real-world data）</li>
<li>被用来测试最后训练处的G（G越相似于Oracle Model，则G越优）</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">START_TOKEN = <span class="number">0</span> <span class="comment">#initial token</span></span><br><span class="line">target_params = cPickle.load(open(<span class="string">'save/target_params.pkl'</span>)) <span class="comment"># target lstm 网络参数</span></span><br><span class="line">target_lstm = TARGET_LSTM(vocab_size, BATCH_SIZE, EMB_DIM, HIDDEN_DIM, SEQ_LENGTH, START_TOKEN, target_params)</span><br><span class="line">generate_samples(sess, target_lstm, BATCH_SIZE, generated_num, positive_file)</span><br></pre></td></tr></table></figure>
<p>网络结构和Generator相同，网络参数是预先提供好的（参考论文中的说法，是利用正态分布随机生成的）</p>
<h1 id="代码流程"><a href="#代码流程" class="headerlink" title="代码流程"></a>代码流程</h1><ol>
<li><p>利用Oracle Model生成训练样本集合{P}</p>
</li>
<li><p>利用{P}预训练G（损失函数是negative log likelihood），训练120轮</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.pretrain_loss = -tf.reduce_sum(tf.one_hot(tf.to_int32(tf.reshape(self.x, [<span class="number">-1</span>])), self.num_emb, <span class="number">1.0</span>, <span class="number">0.0</span>) * tf.log(tf.clip_by_value(tf.reshape(self.g_predictions, [<span class="number">-1</span>, self.num_emb]), <span class="number">1e-20</span>, <span class="number">1.0</span>))) / (self.sequence_length * self.batch_size)</span><br></pre></td></tr></table></figure>
<p>​</p>
</li>
<li><p>利用预训练好的G生成负样本集合{N}，利用{P}和{N}预训练D，训练50大轮（每轮都重新生成负样本，训练3小轮）</p>
</li>
<li><p>初始化roll-out网络</p>
</li>
<li><p>每训练一轮G，更新一次roll-out网络的参数，训练5大轮D（每轮都重新生成负样本，训练3小轮）。重复步骤5一共200次</p>
</li>
</ol>
<h1 id="公式推导"><a href="#公式推导" class="headerlink" title="公式推导"></a>公式推导</h1><p>G输出的一个token的概率分布，theta是G的参数：<script type="math/tex">G_{\theta}(y_t|Y_{1:t-1})</script></p>
<p>固定D后，D反馈给的G的reward，用于更新G的参数：<script type="math/tex">J(\theta)=E[R_T|s, \theta]=\sum_{a}G_{\theta}(a|s)Q_{D_\phi}(s, a)</script></p>
<p>现在的问题在于计算（估计）：<script type="math/tex">Q_{D_\phi}(s=Y_{1:t-1}, a=y_t)</script></p>
<p>当<script type="math/tex">t=T</script>时：<script type="math/tex">Q_{D_\phi}(s=Y_{1:T-1}, a=y_T)=D_{\phi}(Y_{1:T})</script></p>
<p>当$t&lt;T$时，先用roll-out把<script type="math/tex">Y_{1:t}</script>补齐：<script type="math/tex">\{Y^{1}_{1:T}, Y^{2}_{1:T}, ..., Y^{N}_{1:T}\}=MC^{G_{\theta}}(Y_{1:t}, N)</script></p>
<p>再利用上式：<script type="math/tex">Q_{D_\phi}(s=Y_{1:t-1}, a=y_t)=\frac{1}{N}\sum_{n} D_{\phi}(Y^{n}_{1:T})</script></p>
<p>最后再对$J(\theta)$求导进行梯度下降：</p>
<script type="math/tex; mode=display">\nabla_{\theta}J(\theta)=\sum^{T}_{t=1}\sum_{y_t}\nabla_{\theta}G_{\theta}(y_t|Y_{1:1:t-1})Q_{D_{\phi}}(Y_{1:t-1}, y_t)</script><script type="math/tex; mode=display">=\sum^{T}_{t=1}\sum_{y_t}G_{\theta}(y_t|Y_{1:1:t-1})\nabla_{\theta}logG_{\theta}(y_t|Y_{1:1:t-1})Q_{D_{\phi}}(Y_{1:t-1}, y_t)</script><script type="math/tex; mode=display">=\sum^{T}_{t=1}E[\nabla_{\theta}logG_{\theta}(y_t|Y_{1:1:t-1})Q_{D_{\phi}}(Y_{1:t-1}, y_t)]</script><h1 id="实际应用中的问题"><a href="#实际应用中的问题" class="headerlink" title="实际应用中的问题"></a>实际应用中的问题</h1><p>我需要用我们自有的真实数据替代Oracle Model生成的”真实数据“，因此会遇到以下问题：</p>
<ol>
<li>真实数据的数据量是不是足够训练GAN</li>
<li>生成的词语不会超出输入的真实数据的词汇集合 — 即是优点也是缺点</li>
<li>原模型中判断生成数据质量的方法不再可用 — 人工审核、BLEU等</li>
<li><strong>自有的真实数据中句子的长度不再固定，我们需要把它切割成相同长度的序列（是否有影响）</strong> </li>
<li><strong>生成器生成的的句子长度都是相同的，是否能够满足我们的需求（句子生成的终点判定）</strong></li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-tags" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://wangyp.tech/2017/12/22/professional  text classification based on CNN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Wang Yipeng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yipeng's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/12/22/professional  text classification based on CNN/" itemprop="url">基于卷积神经网络（CNN）的专业文本语料分类</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-12-22T09:14:24+08:00">
                2017-12-22
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>T 平台上有大量的双语句对，大部分句对没有被标注类别，其余的句对虽然被标注了类别，但是不准确，真正被准确标注的句对只占相当少的一部分。T 平台上的语料检索、内容分发与路由都依赖于准确的语料类别标注，对于这样的大量的（千万量级）双语句对语料，我考虑利用 CNN 训练出分类器模型，对所有现有的语料数据进行分类，另外可以把分类分类器模型包装成 web 服务，对新增预料数据进行实时的分类。</p>
<h1 id="训练样本收集与预处理"><a href="#训练样本收集与预处理" class="headerlink" title="训练样本收集与预处理"></a>训练样本收集与预处理</h1><h2 id="样本来源"><a href="#样本来源" class="headerlink" title="样本来源"></a>样本来源</h2><p>由于平台上的语料数据专门服务于翻译行业，语料中有相当多的专业词汇，与常见的语料数据库（<a href="http://www.sogou.com/labs/resource/list_news.php" target="_blank" rel="noopener">搜狗新闻数据库</a>、<a href="http://www.nlpir.org/?action-viewnews-itemid-103" target="_blank" rel="noopener">复旦新闻数据库</a>等）差异巨大，因此训练所用到的数据均来自于平台上现有数据。</p>
<h2 id="中英文分离"><a href="#中英文分离" class="headerlink" title="中英文分离"></a>中英文分离</h2><p>目的：将双语文本中的中英文分离，分别训练出一个分类器，这样做可以充分利用双语句对的特性，两个分类器可以相互加强，提高分类器的效果。</p>
<p>方案：构建若干个字符集合（英文字母/数字集合、英文标点集合、中文标点集合、空格、英文全角字母/数字集合），利用<code>filter()</code>函数很容易将这些集合中的字符从原始文本中去除/保留，从而实现中英文的分离。去除标点也可以利用 jieba 分词器自带的词性标注功能，标点和其他罕见字符都会标注成 X，因此很容易去除，但因为词性标注功能和用户词典加载功能不兼容，最后没有采用。</p>
<p>用到的库/方法：标准库 string</p>
<pre><code>import string
string.printable
filter(function, iterable)
</code></pre><h2 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h2><p>目的：无论是文本向量化，还是停止词过滤等基于词语的文本操作，都需要先对文本进行分词。</p>
<p>方案：英文分词直接用空格分词即可，中文分词利用 jieba 分词库。</p>
<p>用到的库/方法：<a href="http://www.oss.io/p/fxsjy/jieba" target="_blank" rel="noopener">中文分词库 jieba</a></p>
<pre><code>import jieba
jieba.posseg.cut()
</code></pre><h2 id="停止词过滤"><a href="#停止词过滤" class="headerlink" title="停止词过滤"></a>停止词过滤</h2><p>目的：<a href="https://en.wikipedia.org/wiki/Stop_words" target="_blank" rel="noopener">停止词</a>指的是那些使用频率过高、对语句信息贡献很小的词，这些词对我们的分类任务几乎没有帮助，而且稀释别的具有区分性的词，因此在训练之前要将这些词过滤掉。将停止词数据库放到集合类型<code>set()</code>中去，过滤方法同上不再赘述。</p>
<h2 id="训练样本的提纯"><a href="#训练样本的提纯" class="headerlink" title="训练样本的提纯"></a>训练样本的提纯</h2><p>目的：神经网络的训练允许训练样本中存在噪声，但平台上现有的语料质量依然无法满足要求，有大量的被错误标注的语料，因此我们需要对现有语料进行一次“提纯”。</p>
<p>方案：<a href="http://pinyin.sogou.com/dict/" target="_blank" rel="noopener">搜狗细胞词库</a>/T 平台上有质量非常好的术语数据，利用这些术语数据可以从原始样本中选出质量相对较高的样本。对于一段语料（假设被标注为：医药），利用医药术语数据，我们可以统计出这段语料中出现的医药术语的频数，以及医药术语占这段语料总词数的百分比，综合这两个指标，我们可以判断出这段语料确实是属于“医药”的程度，通过实验测试出合理的阈值，从而选择出高质量的语料数据。</p>
<p>用到的库/方法：<a href="http://radimrehurek.com/gensim/tutorial.html" target="_blank" rel="noopener">自然语言处理库 Gensim</a></p>
<pre><code>from gensim import corpora
corpora.Dictionary()
corpora.Dictionary.token2id
corpora.Dictionary.filter_tokens()
corpora.Dictionary.compactify()
corpora.Dictionary.save()
corpora.Dictionary.doc2bow()
corpora.Dictionary.load()
</code></pre><h2 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h2><p>目的：必须把长度各异的文本处理成长度相同的向量才能够输入 CNN 进行训练。</p>
<p>方案：由于预料数据中的句子长度较短，单句包含信息量少，不利于我们提取特征、区分类别，因此需要将短句组合成较长的（40 词到 50 词）并且长度相近的句段；然后，利用基于词频的词袋（Bag of Words）模型，把句段转化成 one-hot 向量（向量的每一位对应一个词，这一位的数字就是这个词在句段中出现的频数，可以看出向量长度与词典大小相同）；最后，在输入神经网络之前，做一次长度归一化（通过补 0 或者截短，生成长度恰好为 50 的向量）。</p>
<h1 id="CNN-训练"><a href="#CNN-训练" class="headerlink" title="CNN 训练"></a>CNN 训练</h1><h2 id="CNN-结构"><a href="#CNN-结构" class="headerlink" title="CNN 结构"></a>CNN 结构</h2><ul>
<li>Embedding Layer：输入 CNN 的是尺寸为<code>50 * Size of Dictionary</code>的二维向量， 因为字典尺寸很大，所以这些二维向量的尺寸也很大，并且十分系数（含有大量的0），CNN 不善于处理如此高维而且稀疏的向量，因此需要嵌入层把这些向量降维到<code>50 * 200</code>或者 其他合适的尺寸（根据实际计算能力选择），再输入第二层训练更加高级的特征。这一层的作用就相当于一次预训练，其实已经提取出了初级的文本特征。</li>
<li>Dropout Layer：利用 Dropout 方法，减少过拟合现象。</li>
<li>Convolutional Layer：卷积层，提取文本向量局部特征。</li>
<li>Pooling Layer：进一步缩减向量尺寸。</li>
<li>Fully connected Layer (reLu)：加速训练。</li>
<li>Fully connected Layer (softmax)：把向量收缩到 15 维（对应 15 个类别），生成概率分布。</li>
</ul>
<h2 id="CNN-训练结果"><a href="#CNN-训练结果" class="headerlink" title="CNN 训练结果"></a>CNN 训练结果</h2><p>本文实验语料数据共有15个类别，每个类别分别有 50000 左右句对参与训练，使用 tensorflow 机器学习框架、Keras 提供的高级接口，在 GTX1050ti CUDA 加速下，完成单个 CNN 模型的训练需要 30 min，在测试集的上的分类准确率达到 90%。</p>
<h1 id="输出预测结果"><a href="#输出预测结果" class="headerlink" title="输出预测结果"></a>输出预测结果</h1><p>模型训练完成之后，我们进一步在新的样本上进行测试，发现很多样本的预测结果不符合常识判断，而这些样本实际上不属于任何一个已知的类别，我们要将这些样本剔除，于是考虑新增一个 UNCLEAR 类别，专门用于接纳那些不属于任何一个已知类别的样本，这些样本由于不属于任何一个类别，我们推测这样的样本输入 CNN 网络后，输出的概率分布的方差应该是相对较小的，通过实验，发现情况确实如此，我们成功地找到了一个方差阈值，将一些无分类样本分到了 UNLCELEAR 类别，提高了分类结果地可靠性。</p>
<p>由于 CNN 要求输入的向量尺寸相同，我们需要对一些长句子进行截短操作，这回导致语句信息的丢失，如果丢失了那些能够决定句子类别的关键词语，这将导致预测分类的结果变差，因此，我们对于对同一个样本，我们将词序打乱，用 CNN 进行多次判断，这样因为截短操作而丢失关键信息的可能性就大大减小了，最后我们可以根据多次判断的结果，综合得出预测的结果（如果一个句子多次被判断为同一类别，我们就确信它属于这个列别，其余的可以被预测为 UNCLEAR）。</p>
<p>最终，两步优化大大改善了预测结果的可靠性。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-tags" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://wangyp.tech/2017/09/17/tensorflow instruction/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Wang Yipeng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yipeng's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/09/17/tensorflow instruction/" itemprop="url">Ubuntu 下 Tensorflow-CUDA(GPU加速) 环境搭建</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-09-17T13:34:52+08:00">
                2017-09-17
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文并不是一个教程，step by step 的教网络上有很多，然而我想说的是：turn to offical guide first!!!，我曾了为了搭建 tensorflow-gpu 的环境参考了很多网络上的教程，失败到吐血，各种莫名其妙的错误，最终发现，目前（2017/9）的 tesorflow-gpu 只支持6.X版本cuDNN加速库（我错装成了最新版本，虽然是我蠢，但是很少有教程会强调这点），而所有地细节在官方文档中都能找到明确地答案，所以大家不要偷懒，直接去看官方文档吧，我会在下面贴出所有相关地地址。</p>
<p>因为在 WIN10 上 CUDA 需要 VS2015 运行环境[1]，而我为了开发 UWP 已经安装了 VS2017，所以VS C++ 2015 runtime 一直无法安装，又不想卸载 VS2017，于是就转战 Ubuntu 16.04 LTS 了。</p>
<p>三步走：</p>
<ul>
<li>下载安装 CUDA</li>
<li>下载安装 cuDNN (虽然是选用加速库，但 Tensorflow 要求必须安装，不然报错)</li>
<li>安装 tensorflow-gpu</li>
</ul>
<p><strong>但要先去看 tensorflow 的官方安装指南，了解整个过程与要求。</strong></p>
<h4 id="下载安装-CUDA"><a href="#下载安装-CUDA" class="headerlink" title="下载安装 CUDA"></a>下载安装 CUDA</h4><p><a href="http://docs.nvidia.com/cuda/cuda-installation-guide-linux/#axzz4VZnqTJ2A" target="_blank" rel="noopener">CUDA for Linux 官方文档</a><br><a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="noopener">CUDA 下载地址</a><br>你知道应该选择哪个进行下载</p>
<h4 id="下载安装-cuDNN"><a href="#下载安装-cuDNN" class="headerlink" title="下载安装 cuDNN"></a>下载安装 cuDNN</h4><p><a href="http://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#installlinux" target="_blank" rel="noopener">cuDNN 官方文档</a><br><a href="https://developer.nvidia.com/rdp/cudnn-download" target="_blank" rel="noopener">cuDNN 下载地址</a><br>千万不要下载错了</p>
<h4 id="安装-tensorflow-gpu"><a href="#安装-tensorflow-gpu" class="headerlink" title="安装 tensorflow-gpu"></a>安装 tensorflow-gpu</h4><p><a href="https://www.tensorflow.org/install/" target="_blank" rel="noopener">tensorflow 官方安装指南</a>(需要翻墙…)<br>另外tensorflow官网下又论坛，可以解决很多实际操作中遇到的问题。</p>
<p>最后如果你运行并通过了 tensorflow 官方安装指南下的测试程序，看到你的显卡信息，基本上就算是成功了。</p>
<p>[1] 实际上，后来我又在 WIN10 下尝试搭建环境，虽然 CUDA 在安装的时候警告说找不到支持的编译器 (VS2015)，但依然可以强行安装，最后也可以通过 tensorflow 的测试程序，并且我用 Python 跑了一个 CNN 确实可以加速，难道是因为 anaconda 中自带的 VS2015 runtime… 反正能用就好了</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          
            <img class="site-author-image" itemprop="image"
              src="/images/avatar.jpg"
              alt="Wang Yipeng" />
          
            <p class="site-author-name" itemprop="name">Wang Yipeng</p>
            <p class="site-description motion-element" itemprop="description">Management of skills and knowledge.</p>
        </div>

        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
            
              <a href="/archives/">
            
                <span class="site-state-item-count">4</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">5</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Wang Yipeng</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div>

  <span class="post-meta-divider">|</span>

  <div class="theme-info">Theme &mdash; <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.2</div>


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.2"></script>



  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>


  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  




	





  





  








  





  

  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
